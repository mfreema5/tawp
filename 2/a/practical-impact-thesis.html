<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Practical Impact - a thesis</title>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <link href="css/reset.css" media="screen,print" rel="stylesheet">
    <script src="css/modernizr.custom.99475.js"></script>
    <link href="css/screen.css" rel="stylesheet">
    <link href="css/gfx/favicon.ico" rel="icon" type="image/x-icon">
  </head>
  <body>
    <main>
      <h1>
        Practical impact &ndash; a thesis
      </h1>
      <article id="top">
        <h2>
          &nbsp;
        </h2>
        <section id="preface-document-structure">
          <h3>
            Preface: document structure
          </h3>
          <p>
            The structure of this thesis roughly maps to the chain of reasoning that I followed when considering the issues that led to and arise from the notion of tracking the “practical impact” of research.  I started with the problem of the common misapplication of statistical methods in research in management studies.  That problem, however, is really a just one part of the more general issue of research that lacks relevance outside of academia, not only in the management literature, but in other social science as well.
          </p>
          <p>
            Since there are already processes in place to identify the value of research to other researchers&mdash;citation metrics&mdash;I wondered if similar processes couldn't be developed to identify the value of research to practitioners. <span class="t-text" id="tog29"> Such a process would need to involve two parts: 1 - finding appropriate information, and 2 - applying that information in an appropriate manner.  I examined the existing metrics for measuring the “scientific” (as opposed to “practical”) value of research to see what lessons could be learned about gathering information for a future practical impact metric.  I also examined how current metrics are used, in addition to researching previous proposals (and subsequent discussions) about including practice in the evaluation of research, to gain insights into how to best apply the output of a practical impact metric. </span><span class="audience-p">&nbsp; <button class="toggle" onclick="toggleSpan('tog29');"> ± </button> &nbsp;</span>
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="null-hypothesis-significance-testing">
        <h2>
          Null-hypothesis significance testing
        </h2>
        <section id="losing-importance-in-the-search-for-significance">
          <h3>
            Losing importance in the search for significance
          </h3>
          <p>
            Null-hypothesis significance testing (NHST) is a well-established and widespread statistical technique.  It allows a researcher to calculate the probability that variables which appear to be correlated actually are correlated.  When there is a high probability that a correlation is real&mdash;as opposed to the result of random chance&mdash;that correlation is given the rather unfortunately-chosen description of “statistically significant.”
          </p>
          <aside class="audience-p">
            <h4>
              Regression and error
            </h4>
            <p>
              Multiple regression analysis operates on the mean values (the average) of variables.  Typically, the data being used in the analysis is from a sample, and does not represent the entire population.  Therefore, there is an error associated with the mean for each variable in the sample.  This is because the mean of the sample likely differs from mean of the population.
            </p>
            <p>
              The error in the means causes an error in the regression analysis.  If a correlation is at the edge of the ability of an analysis to detect, the error that propagates from the sample means may shift the regression analysis result over that edge-of-detection.  The result might be pushed over the edge in either direction, i.e., from not-correlated to correlated, or from correlated to not-correlated.
            </p>
            <p>
              If the regression analysis result is pushed from not-correlated to correlated, that's a Type I error.
            </p>
            <p>
              If the regression analysis result is pushed from correlated to not-correlated, that's a Type II error.
            </p>
            <ul>
              <li>
                Correlation exists and is detected: No error.
              </li>
              <li>
                Correlation does not exist and is detected: Type I error.
              </li>
              <li>
                Correlation exists and is not detected: Type II error.
              </li>
              <li>
                Correlation does not exist and is not detected: No error.
              </li>
            </ul>
            <p>
              Null-hypothesis significance testing finds the probability of a Type I error.  It is dependent on sample size, because the error in the means of the sample is also dependent on sample size.
            </p>
            <p>
              For more background multiple regression analysis and NHST, see the online short course <a href="http://www.unt.edu/rss/class/Jon/ISSS_SC/Module010/isss_m10_1_simpleReg/">&ldquo;Simple (bivariate) Regression&rdquo;</a> <a href="references.html#Starkweather2010">(Starkweather 2010)</a>.
            </p>
          </aside>
          <p>
            It's an unfortunate label because it's easy shortened to “significant”, and mistaken for a judgement on the importance of a empirical results, as opposed to being only a statement about the chances of it being in error.
          </p>
          <p>
            Researchers can unconsciously exploit this confusion to their advantage by focusing on research that results in high probabilities of statistical significance and is therefore likely to be published, and avoiding research that is potentially of great importance, but that doesn't have a high probability of statistical significance.
          </p>
          <p>
            For example, in an editorial in the <i>Academy of Management Journal</i>, Combs questioned whether the ever-more-easy availability of large data sets couple with the ease of performing NHST afford by modern computers and software combine to:
          </p>
          <blockquote>
            …mask other shortcomings of our research designs and leave us with itty-bitty effect sizes that limit the relevance of our research. …As management scholars, can we really suggest that managers should change their decision calculus on the basis of knowledge that some new variable explains .0025 percent of the variance in organizational performance? <a onclick="toggleAside('ref97');">(Combs 2010)</a>
          </blockquote>
          <aside class="side-ref" id="ref97">
            <p>
              Combs, James G. 2010. “Big Samples and Small Effects: Let's Not Trade Relevance and Rigor for Power.” <i>Academy of Management Journal</i> 53 (1): 9–13.
            </p>
            <div>
              <a href="references.html#combs2010">references</a>
            </div>
          </aside>
          <p>
            Combs went beyond merely speculating on a shift toward larger data sets with smaller effects, and did some comparisons between studies published in the 1987-89 volumes and in the 2007-8 volumes of the <i>Academy of Management Journal</i>.  During that 20-year period, the mean sample size rose from 300 to 3,423 (excluding the three largest samples from the later period that, if included, would raise the latter mean to 7,578).  Meanwhile, the mean correlations fell from .22, to .17.  The larger samples made smaller and smaller correlations statistically significant <a onclick="toggleAside('ref97');">(Combs 2010)</a>.
          </p>
          <p>
            Ellis did a similar survey of empirical studies in the <i>Journal of International Business Studies</i> and found a similar shift toward larger sample sizes, with a similar detrimental effect on relevance of results:
          </p>
          <blockquote>
            In many of the studies I read it was apparent that researchers confused statistical with substantive significance when interpreting their results. This usually happened when conclusions about effects were drawn solely by looking at the p-values of test results. The problem with this is that the p-values generated by statistical significance tests are confounded indices that reflect both the size of the effect as it occurs in the population and the sample size used to detect it. As N goes up p goes down, irrespective of the underlying effect size. The implication is that trivial results are sometimes interpreted as meaningful in large-N studies…. <a onclick="toggleAside('ref122');">(Ellis 2010)</a>
          </blockquote>
          <aside class="side-ref" id="ref122">
            <p>
              Ellis, Paul D. 2010. “Effect Sizes and the Interpretation of Research Results in International Business.” <i>Journal of International Business Studies</i> 41 (9). Nature Publishing Group: 1581–88. <a href="http://doi.org/10.1057/jibs.2010.39" >10.1057/jibs.2010.39</a>.
            </p>
            <div>
              <a href="references.html#ellis2010">references</a>
            </div>
          </aside>
          <p>
            <a onclick="toggleAside('ref133');">Lockett <i>et al.</i> (2014)</a> note that the problem of misapplying NHST is not limited to managent research: “Commentators have detailed the problems of employing NHST across a range of different social science disciplines including management, economics and psychology". And, more so, the concerns are not even restricted to the social sciences, as there are “similar concerns in medicine, natural sciences and engineering”.
          </p>
          <aside class="side-ref" id="ref133">
            <p>
              Lockett, Andy, Abagail McWilliams, and David D. Van Fleet. 2014. “Reordering Our Priorities by Putting Phenomena before Design: Escaping the Straitjacket of Null Hypothesis Significance Testing.” <i>British Journal of Management</i> 25 (4): 863–73. <a href="http://doi.org/10.1111/1467-8551.12063">10.1111/1467-8551.12063</a>.
            </p>
            <div>
              <a href="references.html#lockett2014">references</a>
            </div>
          </aside>
          <p>
            Lockett, <i>et al.</i>, also point out that NHST has become central to the question of whether or not a paper that presents an empirical study will be published since statistical significance is often (incorrectly) used a proxy for importance. <span class="t-text" id="tog73">As they put it: NHST “is commonly employed as the sole criterion in deciding on whether or not an effect size is (statistically) significant, with no real consideration of whether or not the effect size is substantively significant (i.e. it really matters)” <a onclick="toggleAside('ref133');">(Lockett <i>et al.</i> 2014)</a>.</span><span class="audience-p">&nbsp; <button class="toggle" onclick="toggleSpan('tog73');"> ± </button> &nbsp;</span>
          </p>
          <p>
            They then close the loop, mentioning that since: A - larger sample sizes make even trivial effects statistically significant; and B - statistical significance is central to whether or not a paper gets published; then it follows that “generating a large sample size is an important factor in determining the probability of a paper being published in a top-rated journal”.  This is in spite the fact that “…the impact of the research may be significantly undermined because the results that are deemed important under the criterion of statistical significance may well be trivial in terms of substantive significance” <a onclick="toggleAside('ref133');">(Lockett <i>et al.</i> 2014)</a>.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="making-importance-once-again-important">
        <h2>
          Making importance once again important
        </h2>
        <section id="demonstrate-importance-through-effect-sizes">
          <h3>
            Demonstrate importance through effect sizes
          </h3>
          <p>
            A corrective action to limit the erosion of the importance of research due to the misapplication of NHST was summarized nicely by Schwab <i>et al.</i> when they wrote that “researchers should show the substantive importance of findings by reporting effect sizes” <a onclick="toggleAside('ref..');" >(Schwab <i>et al.</i> 2011)</a>.
          </p>
          <aside class="side-ref" id="ref156">
            <p>
              Schwab, Andreas, Eric Abrahamson, William H. Starbuck, and Fiona Fidler. 2011. “PERSPECTIVE—Researchers Should Make Thoughtful Assessments Instead of Null-Hypothesis Significance Tests.” <i>Organization Science</i> 22 (4): 1105–20. <a href="http://doi.org/10.1287/orsc.1100.0557" >10.1287/orsc.1100.0557</a>.
            </p>
            <div>
              <a href="references.html#schwab2011">references</a>
            </div>
          </aside>
          <p>
            Using effects size to supplant the importance afforded to NHST comes up repeatedly in the review by <a onclick="toggleAside('ref167');">Fidler <i>et al.</i> (2004)</a> of various statistical reforms in medicine, psychology and other fields.  For example, they report that the guidelines from an APA Task Force on Statistical Inference for publishing statistical results “strongly emphasised the need for measures of effect size to be the primary outcome of a study, and the need to differentiate clinical or practical significance from statistical significance”.
          </p>
          <aside class="side-ref" id="ref167">
            <p>
              Fidler, Fiona, Cumming Geoff, Burgman Mark, and Thomason Neil. 2004. “Statistical Reform in Medicine, Psychology and Ecology.” <i>The Journal of Socio-Economics</i> 33 (5): 615–30. <a href="http://doi.org/10.1016/j.socec.2004.09.035" >10.1016/j.socec.2004.09.035</a>.
            </p>
            <div>
              <a href="references.html#fidler2004">references</a>
            </div>
          </aside>
        </section>
        <section id="from-calculations-to-precedents">
          <h3>
            From calculations to precedents
          </h3>
          <p>
            However, while moving from relying on statistical significance alone to instead using statistical significance in combination with determinations of the effect size is conceptually easy, in practice it can be a major change.
          </p>
          <p>
            For example, consider the complications that Fidler <i>et al.</i> detail to explain why the move to effect sizes that was an effective method of statistical reform in medical research, failed to take hold in psychology:
          </p>
          <blockquote>
            One often-suggested solution is to standardise the effect sizes, that is, report the effect in units of standard deviation. However, it is important to recognise that CIs for standardized measures are not straightforward. They typically require non-central distributions, something most psychologists have never encountered. Also, they involve highly intensive computing processes that have only recently been accessible to individual researchers.<br /><br />These are not problems medicine encountered. The conceptual shift to estimation was relatively straightforward and so were CI calculations for what became the effect sizes of choice, odds ratios and relative risk values. <a onclick="toggleAside('ref189');">(Fidler <i>et al.</i> 2004)</a>
          </blockquote>
          <aside class="side-ref" id="ref189">
            <p>
              Fidler, Fiona, Cumming Geoff, Burgman Mark, and Thomason Neil. 2004. “Statistical Reform in Medicine, Psychology and Ecology.” <i>The Journal of Socio-Economics</i> 33 (5): 615–30. <a href="http://doi.org/10.1016/j.socec.2004.09.035" >10.1016/j.socec.2004.09.035</a>.
            </p>
            <div>
              <a href="references.html#fidler2004">references</a>
            </div>
          </aside>
        </section>
        <section id="using-precedents-as-estimates">
          <h3>
            Using precedents as estimates
          </h3>
          <p>
            However, there may be something of a shortcut.  What Fidler <i>et al.</i> are describing is a mathematically rigorous determination of the effect size of a research result.
          </p>
          <p>
            But is that what is needed?  The goal is simply to ensure that the size of the effect being reported in a study isn't irrelevant.  Or, to come at it from the other direction, what is needed is some estimation that the effect size is sufficiently large enough to be relevant to practitioners.
          </p>
          <p>
            We can possibly employ the same approach that is used when designing a statistical study.  Determining of the statistical power requirements of a study requires that you know the effect size, yet the effect size is typically one of the things you'll be measuring with the study.  Therefore, what is commonly done is to estimate the effect size, based on previous research.
          </p>
          <p>
            So, we can estimate what sort of effect size will be of interest to practitioners by looking at what effect sizes they have previously found interesting.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="simplify-the-problem">
        <h2>
          Simplify the problem…
        </h2>
        <section id="by-expanding-it">
          <h3>
            …By expanding it
          </h3>
          <p>
            Up to this point the type of research in question has been rather narrow: empirical studies that employ NHST.
          </p>
          <p>
            But that is not the only sort of research that might be relevant to practitioners.  More importantly, that is not the only type of research that has the potential to be relevant to practitioners, but is failing to fullfil that potential.
          </p>
          <aside class="side-ref" id="ref234">
            <p>
              Rynes, Sara L, and Jean M Bartunek. 2001. “Across the Great Divide: Knowledge Creation and Transfer between Practitioners and Academics.” <i>Academy of Management Journal</i> 44 (2): 340–55. <a href="http://www.jstor.org/stable/3069460" >http://www.jstor.org/stable/3069460</a>.
            </p>
            <div>
              <a href="references.html#rynes2001">references</a>
            </div>
          </aside>
          <p>
            For example, <a onclick="toggleAside('ref234');">Rynes <i>et al.</i> (2001)</a> were not speaking only of empirical studies that use NHST when they wrote that, “A substantial body of evidence suggests that executives typically do not turn to academics or academic research findings in developing management strategies and practices”.
          </p>
          <p>
            Also, as part of his incoming Presidential Address to the Academy of Management—which had the unsubtle title of “What If the Academy Actually Mattered?”—<a onclick="toggleAside('ref248');">Hambrick (1993)</a> wrote about the general gap between researchers and practitioners in that field:
          </p>
          <aside class="side-ref" id="ref248">
            <p>
              Hambrick, Donald C. 1994. “What If the Academy Actually Mattered?” <i>The Academy of Management Review</i> 19 (1): 11–16. <a href="http://www.jstor.org/stable/258833">http://www.jstor.org/stable/258833</a>.
            </p>
            <div>
              <a href="references.html#hambrick1994">references</a>
            </div>
          </aside>
          <blockquote>
            Rather, we simply seem to have a minimalist ethos: minimal programming, minimal dues, minimal staff, minimal innovation, minimal visibility, minimal impact. Each August, we come to talk with each other; during the rest of the year we read each others' papers in our journals and write our own papers so that we may, in turn, have an audience the following August: an incestuous, closed loop.
          </blockquote>
          <p>
            There would seem to be no reason to we must restrict ourselves to asking “What effect sizes, in which correlations, would a practitioner care about?”  The failure to achieve relevance is more general than that, so we can ask a more general question: “What research would a practitioner care about?”
          </p>
        </section>
        <section id="find-out-from-practitioners-what-they-consider-relevant">
          <h3>
            Find out from practitioners what they consider relevant
          </h3>
          <p>
            The obvious way to get an answer to the more general question proposed in the previous section would be to pose it directly to a practitioner: “What research do you care about?"
          </p>
          <p>
            It turns out that there is a precedent this sort of question.  Almost one hundred years ago, Gross and Gross posed the question:
          </p>
          <blockquote>
            What… scientific periodicals are needed in a college library successfully to prepare the student for advanced work, taking into consideration also those materials necessary for the stimulation and intellectual development of the faculty? <a onclick="toggleAside('ref276');">(Gross and Gross 1927)</a>
          </blockquote>
          <aside class="side-ref" id="ref276">
            <p>
              Gross, P. L. K., and E. M. Gross. 1927. &ldquo;College Libraries and Chemical Education.&rdquo; <i>Science</i> 66 (1713): 385–89.
            </p>
            <div>
              <a href="references.html#gross1927">references</a>
            </div>
          </aside>
          <p>
            What makes it similar to our question about what research would be of interest to practitioners is that “one way to answer this question would be merely to sit down and compile a list of those journals which one considers indispensable” <a onclick="toggleAside('ref276');">(Gross and Gross, 1927)</a>.  In other words, the first way that comes to mind to address the question is to go and ask the subject of the question.
          </p>
          <p>
            However, as <a onclick="toggleAside('ref276');">Gross and Gross (1927)</a> go on to explain, while you might get good results from a direct inquiry, “it seems reasonably certain that often the result would be seasoned too much by the needs, likes and dislikes of the compiler.”
          </p>
          <p>
            What they ended up doing instead of directly asking the subject, was to collect data on the subject's behavior, and extrapolate from that what journals the subject finds “indispensable”.  Specifically, they looked at citations in and about journals; and started the chain of research that led to the Journal Impact Factor and all the myriad other citation-based metrics of research.
          </p>
          <p>
            Following that example, in order to discover what research practitioners care about, we should find out what research is being used by practitioners, and what research are they discussing among themselves.
          </p>
        </section>
        <section id="specifics-of-what-would-be-tracked">
          <h3>
            Specifics of what would be tracked
          </h3>
          <p>
            When practitioners use research results in their practice, or discuss them with other practitioners, we'll say that the research had a “practical impact”, to distinguish from “scientific impact” or “citation impact”.
          </p>
          <p>
            Studies in the area of altmetrics have demonstrated the viability of tracking at least some of the research-relevance-indicating behaviors of practitioners, along with tracking what <a onclick="toggleAside('ref307');">Adie and Roe (2013)</a> called “mentions”, that is, links to research documents in main-stream media sources, and social media discussions. For specific examples, see the section below on “Altmetrics.”
          </p>
          <aside class="side-ref" id="ref307">
            <p>
              Adie, Euan, and William Roe. 2013. “Altmetric: Enriching Scholarly Content with Article-Level Discussion and Metrics.” <i>Learned Publishing</i> 26 (1): 11–17. <a href="http://doi.org/10.1087/20130103" >10.1087/20130103</a>.
            </p>
            <div>
              <a href="references.html#adie2013">references</a>
            </div>
          </aside>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="existing-metrics">
        <h2>
          Existing metrics
        </h2><span class="t-text" id="tog204">
        <p class="lightbox">
          There exists a variety of metrics for judging the scientific quality research.  How those metrics are constructed and used can should inform any metric intended to track the practical impact of research.
        </p>
        <section id="journal-level-citation-metrics">
          <h3>
            Journal-level citation metrics
          </h3>
          <p>
            All citation-based metrics are typically traced back to the paper by Gross and Gross (1927) mentioned above.  <a onclick="toggleAside('ref335');" >Archambault and Larivière (2009)</a> tap it as the starting point for the Journal Impact Factor, the best known and possibly most widely used citation-based metric for research.
          </p>
          <aside class="side-ref" id="ref335">
            <p>
              Archambault, Éric, and Vincent Larivière. 2009. “History of the Journal Impact Factor: Contingencies and Consequences.” <i>Scientometrics</i> 79 (3): 635–49. <a href="http://doi.org/10.1007/s11192-007-2036-x" >10.1007/s11192-007-2036-x</a>.
            </p>
            <div>
              <a href="references.html#archambault2009">references</a>
            </div>
          </aside>
          <p>
            As an example of citation-based metrics in general, I'll trace the origins of the Journal Impact Factor, drawing heavily on &lsquo;Archambault, Éric, and Vincent Larivière. 2009. “History of the Journal Impact Factor: Contingencies and Consequences.” <i>Scientometrics</i> 79 (3): 635–49. <a href="http://doi.org/10.1007/s11192-007-2036-x" >10.1007/s11192-007-2036-x</a>&rsquo;.
          </p>
        </section>
        <section id="finding-the-journals">
          <h3>
            Finding the &ldquo;indispensable&rdquo; journals
          </h3>
          <p>
            In 1927, as mentioned above, <a onclick="toggleAside('ref354');">Gross and Gross (1927)</a> posed the question, “What… scientific periodicals are needed in a college library?”
          </p>
          <aside class="side-ref" id="ref354">
            <p>
              Gross, P. L. K., and E. M. Gross. 1927. &ldquo;College Libraries and Chemical Education.&rdquo; <i>Science</i> 66 (1713): 385–89.
            </p>
            <div>
              <a href="references.html#gross1927">references</a>
            </div>
          </aside>
          <p>
            To answer that question of to which journals a library should subscribe, they used quantitative methods to make comparisons between journals.  Their methods eventually led to the Journal Impact Factor, and other related bibliometrics.
          </p>
          <p>
            Gross and Gross used a quantitive method to evaluate journals, as opposed to merely asking an expert to compile a list of &ldquo;indispensable&rdquo; journals in order to avoid, as they described it, a list that was &ldquo;seasoned too much by the needs, likes and dislikes of the compiler.&rdquo;  In other words, their goal was to achieve objectivity in the evaluation by using quantitative methods.
          </p>
        </section>
        <section id="counting-citations-to-sort-journals">
          <h3>
            Counting citations to sort journals
          </h3>
          <p>
            The basic method pioneered by Gross and Gross, and repeated later by other researchers, was to select a pre-eminent journal or a reference that was generally accepted as key to a field, and then to compile and quantify the sources of the citations in that keystone reference/journal.
          </p>
          <p>
            An extremely simplified example: if the journal that was selected as the central reference for a field, &ldquo;Field&rdquo;, contained five citations to a &ldquo;Journal A&rdquo;, ten citations to a &ldquo;Journal B&rdquo;, and four citations to a &ldquo;Journal C&rdquo;, then for that field the journals would be ranked and reported something like this:
          </p>
          <ul>
          <li>Leading periodicals in Field

          <ul>
          <li>Journal B &ndash; 10</li>
          <li>Journal A &ndash; 5</li>
          <li>Journal C &ndash; 4</li>
          </ul></li>
          </ul>
          <p>
            These early citation metrics were inherently field-specific, since they took data from a central reference to some particular field.
          </p>
        </section>
        <section id="compiling-citations-for-multiple-fields">
          <h3>
            Compiling citations for multiple fields
          </h3>
          <p>
            Soon, though researchers began to cross-compile citation information from multiple journals, and then to include journals from multiple fields in their compilations.  For example, <a onclick="toggleAside('ref399');">Gregory (1937)</a> compiled more than 27,000 citations from across 27 subfields in medicine.
          </p>
          <aside class="side-ref" id="ref399">
            <p>
              Gregory, Jennie. 1937. “An Evaluation of Medical Periodicals.” <i>Bulletin of the Medical Library Association</i> 25 (3): 172–88. <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC234080/" >PMC234080"</a>.
            </p>
            <div>
              <a href="references.html#gregory1937">references</a>
            </div>
          </aside>
          <p>
            Gregory gives a concise explanation of the purpose of her metrics:
          </p>
          <blockquote>
            The foregoing Tables (I-27) answer primarily the needs of the specialist in his attempt to keep adequately abreast of the literature in his field. Two further Tables have been compiled, to indicate to the medical library and the librarian (A), the indispensable periodicals for al the fields consulted; (B), a short list of essential periodicals in general medicine which cover a large amount of material. This latter list is designed primarily for the individual and for the small library.
          </blockquote>
          <p>
            There were 27 &ldquo;foregoing Tables&rdquo;, one for each of the subfields.  No comparisons were made between those fields.  This is a feature of similar metrics that would continue for decades: even when multiple fields were measured at the same time, the results for each field were reported separately.  The intended audience for these metrics had no need for cross-field comparisons.  Libraries are generally tasked to select the best journals for some particular field, and are not given carte blanche to choose which fields to support.  Similarly, researchers are interested in their field, not all fields.
          </p>
          <aside class="side-ref" id="ref416">
            <p>
              Martyn, John, and Alan Gilchrist. 1968. <i>An Evaluation of British Scientific Journals</i>. London: Aslib.
            </p>
            <div>
              <a href="references.html#martyn1968">references</a>
            </div>
          </aside>
          <p>
            The compilation of citations from <a onclick="toggleAside('ref416');" >Martyn and Gilchrist (1968)</a> is cited by <a onclick="toggleAside('ref427');" >Archambault and Larivière (2009)</a> as having a large influence on methods used in the Journal Impact Factor, and it continues the practice of separating the reporting of metrics for separate fields.
          </p>
          <aside class="side-ref" id="ref427">
            <p>
              Archambault, Éric, and Vincent Larivière. 2009. “History of the Journal Impact Factor: Contingencies and Consequences.” <i>Scientometrics</i> 79 (3): 635–49. <a href="http://doi.org/10.1007/s11192-007-2036-x" >10.1007/s11192-007-2036-x</a>.
            </p>
            <div>
              <a href="references.html#archambault2009">references</a>
            </div>
          </aside>
        </section>
        <section id="reporting-ratios-instead-of-counts">
          <h3>
            Reporting ratios instead of counts
          </h3>
          <p>
            Another feature of the Martyn and Gilchrist compilation, was was carried forward into the Journal Impact Factor and other bibliometrics, was first proposed by <a onclick="toggleAside('ref443');">Hackh (1936)</a>: the reporting of a ratio of the number of citations to the number of pieces that might be cited.
          </p>
          <aside class="side-ref" id="ref443">
            <p>
              Hackh, Ingo. 1936. &ldquo;The Periodicals Useful in the Dental Library.&rdquo; <i>Bulletin of the Medical Library Association</i> 25 (1-2): 109–12. <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC234105/">PMC234105</a>.
            </p>
            <div>
              <a href="references.html#hackh1936">references</a>
            </div>
          </aside>
          <p>
            Another extremely simplified example: if there were five citations to articles published in &ldquo;Journal A&rdquo; in the same year that &ldquo;Journal A&rdquo; published a total of 20 articles, the citation ratio would be &ldquo;5:20&rdquo;, or &ldquo;0.25&rdquo;.
          </p>
          <p>
            We can extend our example to include comparing journals.
          </p>
          <table>
            <thead>
              <tr>
                <th>Journal</th>
                <th>Citation count</th>
                <th>Citable pieces</th>
                <th>Ratio</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Journal A</td>
                <td>5</td>
                <td>20</td>
                <td>0.25</td>
              </tr>
              <tr>
                <td>Journal B</td>
                <td>10</td>
                <td>20</td>
                <td>0.50</td>
              </tr>
              <tr>
                <td>Journal C</td>
                <td>4</td>
                <td>10</td>
                <td>0.40</td>
              </tr>
            </tbody>
          </table>
          <ul>
          <li>Leading periodicals in Field

          <ul>
          <li>Journal B &ndash; 0.50</li>
          <li>Journal C &ndash; 0.40</li>
          <li>Journal A &ndash; 0.25</li>
          </ul></li>
          </ul>
          <p>
            This is the core of how the Journal Impact Factor and a number of similar metrics work.
          </p>
        </section>
        <section id="selecting-what-years-to-include">
          <h3>
            Selecting what years to include
          </h3>
          <p>
            There is an important, but subtle, characteristic of compilations of citations left to be described, which has been carried forward all the way from the earliest forms to the Journal Impact Factor and other contemporary metrics.  But to explain it, we must first look at an important but obvious charactertisic: the time &ldquo;window&rdquo; within which citations must occur in order to be included in a compilation's counts.
          </p>
          <p>
            From the begining there was a time restriction around the citations included.  The <a onclick="toggleAside('ref510');">Gross and Gross (1927)</a> compilation only considered citations from 1926, because they drew all of their citations from the 1926 volume of <i>The Journal of the American Chemical Society</i>, which at the time was &ldquo;the most recent complete volume&rdquo;.  Other early metrics used restricted windows of time for similarly practical reasons.
          </p>
          <aside class="side-ref" id="ref510">
            <p>
              Gross, P. L. K., and E. M. Gross. 1927. &ldquo;College Libraries and Chemical Education.&rdquo; <i>Science</i> 66 (1713): 385–89.
            </p>
            <div>
              <a href="references.html#gross1927">references</a>
            </div>
          </aside>
          <p>
            <a onclick="toggleAside('ref521');">Martyn and Gilchrist (1968)</a> also give a practical reason for the restricted window of time in their citation metrics: &ldquo;the two-year sample reduced the effort of counting and also reduced the cost of acquisition of the data&rdquo;.  But they preface that with an interesting justification:
          </p>
          <aside class="side-ref" id="ref521">
            <p>
              Martyn, John, and Alan Gilchrist. 1968. <i>An Evaluation of British Scientific Journals</i>. London: Aslib.
            </p>
            <div>
              <a href="references.html#martyn1968">references</a>
            </div>
          </aside>
          <blockquote>
            We decided that our most practical course would be to confine our study to citations made during 1965 to journals published in the two preceding years. It was already known that 26.1% of all the 1965 citations were to literature of 1964 and 1963, so in terms of number of citations this would give us an adequate sample.
          </blockquote>
          <p>
            Their method was not meant to be comprehensive.  In other words, it did not capture the entire population of citations, or pieces that could be cited.  Instead, the method operates on a sample of citations, and publications.
          </p>
        </section>
        <section id="samples-not-populations-of-citations">
          <h3>
            Samples, not populations, of citations
          </h3>
          <p>
            That these metrics are based on samples&mdash;as opposed to entire populations&mdash;is the important but non-obvious characteristic of citation compilations mentioned above.
          </p>
          <p>
            It's an important feature because many of the criticisms of the Journal Impact Factor can be viewed as simply issues related to the <em>sampling method</em> of the metric.  (What time window?  Which journals?  Which published pieces?)  And many of the contemporary bibliometric metrics that are offered as improvements to the Journal Impact Factor primarily differ from it only in the sampling methods employed.  Other than that they return the same citation-based analysis as the Journal Impact Factor.
          </p>
        </section>
        <section id="so-whats-a-jif-anyway">
          <h3>
            So, what's a JIF, anyway?
          </h3>
          <p>
            In short: the Journal Impact Factor, and similar metrics:
          </p>
          <ul>
          <li>Sample citations in various journals</li>
          <li>Compare the number of actual citations to the number of potential citations</li>
          </ul>
        </section>
        <section id="altmetrics-overview">
          <h3>
            Altmetrics - overview
          </h3>
          <p>
            According to <a onclick="toggleAside('ref566');">Zahedi <i>et al.</i> (2014)</a> alternative metrics include, “usage data analysis (download and view counts); web citation, and link analyses.”  An even more general description is that altmetrics are intended to use data available on the web to supplement and improve upon citation-based metrics for measuring the impact of science and research.  Through the access to information made available to the web, altmetrics can go beyond the journal articles and books included in citation-based metrics to include “other outputs such as datasets, software, slides, blog posts, etc.” <a onclick="toggleAside('ref566');">(Zahedi <i>et al.</i> 2014)</a>.
          </p>
          <aside class="side-ref" id="ref566">
            <p>
              Zahedi, Zohreh, Rodrigo Costas, and Paul Wouters. 2014. “How Well Developed Are Altmetrics? A Cross-Disciplinary Analysis of the Presence of ‘alternative Metrics’ in Scientific Publications.” <i>Scientometrics</i> 101 (2): 1491–1513. <a href="http://doi.org/10.1007/s11192-014-1264-0" >10.1007/s11192-014-1264-0</a>.
            </p>
            <div>
              <a href="references.html#zahedi2014">references</a>
            </div>
          </aside>
          <p>
            Instead of compiling citations to research in journals, altmetrics involves compiling “mentions” of research in “main-stream media sources, and social media shares and discussions”, along with statistics on downloads, and reference manager counts (<a onclick="toggleAside('ref577');">Adie and Roe 2013)</a>.
          </p>
          <aside class="side-ref" id="ref577">
            <p>
              Adie, Euan, and William Roe. 2013. “Altmetric: Enriching Scholarly Content with Article-Level Discussion and Metrics.” <i>Learned Publishing</i> 26 (1): 11–17. <a href="http://doi.org/10.1087/20130103" >10.1087/20130103</a>.
            </p>
            <div>
              <a href="references.html#adie2013">references</a>
            </div>
          </aside>
          <p>
            Altmetrics also typically retain the related meta-data of mentions and usage statistics, which allow for more complex analyses of the information.  For example, <a onclick="toggleAside('ref588');">Mohammadi et al. (2014)</a> studied the relationship between citation metrics and the usage statistics and user information available from a reference manager service, and found some insights into the the limitations of citation-based metrics.
          </p>
          <aside class="side-ref" id="ref588">
            <p>
              Mohammadi, Ehsan, Mike Thelwall, Stefanie Haustein, and Vincent Larivière. 2014. “Who Reads Research Articles? An Altmetrics Analysis of Mendeley User Categories.” <i>Journal of the Association for Information Science and Technology</i>, 1–27.
            </p>
            <div>
              <a href="references.html#mohammadi2014">references</a>
            </div>
          </aside>
          <p>
            So, altmetrics not only track what research is being “mentioned”, but also where it is mentioned, and who is mentioning it.
          </p>
        </section>
        <section id="altmetrics-examples-related-to-practical-impact">
          <h3>
            Altmetrics - examples related to practical impact
          </h3>
          <p>
            For an example of possible ways to identify practical impact using altmetrics, we'll return to the study by <a onclick="toggleAside('ref616');">Mohammadi <i>et al.</i> (2014)</a>, in which they analyzed data from Mendeley, an on-line service and software package for managing personal libraries of research papers.
          </p>
          <p>
            What makes the Mendeley data potentially useful for identifying possible practical impact is that the meta-data includes the user's “profession”.  Users self-select that from a pre-populated list of options, so it's not infallible, but it does allow for classing users as being research academics (those who author citable research), non-research academics (those who don't author papers&mdash;e.g., students), or someone outside of academia.  The research preferred by the class of users who are not members of academia would presumably be the research that is most relevant to practitioners.
          </p>
          <p>
            As Mohammadi <i>et al.</i> put it:
          </p>
          <blockquote>
            …It seems that Mendeley readership is able to provide evidence of using research articles in contexts other than for their science contribution, at least for Social Science and some applied sub-disciplines. …It also could be used as a supplementary indicator to measure the impact of some technological or medical papers in applied contexts, as citation analysis is more useful for the assessment of theoretical research rather than applied research. <a onclick="toggleAside('ref616');" >(Mohammadi <i>et al.</i> 2014)</a>
          </blockquote>
          <aside class="side-ref" id="ref616">
            <p>
              Mohammadi, Ehsan, Mike Thelwall, Stefanie Haustein, and Vincent Larivière. 2014. “Who Reads Research Articles? An Altmetrics Analysis of Mendeley User Categories.” <i>Journal of the Association for Information Science and Technology</i>, 1–27.
            </p>
            <div>
              <a href="references.html#mohammadi2014">references</a>
            </div>
          </aside>
          <p>
            Where “contexts other than for their science contribution” and “the impact… in applied contexts” are presumably equivalent to practical impact.
          </p>
          <p>
            Though, we should also note one of the caveats given by the authors: “Mendeley is perhaps most useful for those who will eventually cite an article and so its readership counts seem likely to under-represent users who will never need to cite an article, perhaps including disproportionately many practitioners” <a onclick="toggleAside('ref616');">(Mohammadi <i>et al.</i> 2014)</a>.  So, while the data from Mendeley suggests the potential of tracking practical impact, it may not be able to fulfill it.
          </p>
          <p>
            Similarly, the various examples of altmetric data given by <a onclick="toggleAside('ref633');" >Adie and Roe (2013)</a> hint at the potential for extracting practical impact information from social media sources, while also highlighting some of the difficulties in utilizing those sources.  E.g., though they were able to collect meta-data about the users involved in “mentions” of citable research, there's no indication that there is any data available which would allow users to be classified as being, or not being, members of academia.
          </p>
          <aside class="side-ref" id="ref633">
            <p>
              Adie, Euan, and William Roe. 2013. “Altmetric: Enriching Scholarly Content with Article-Level Discussion and Metrics.” <i>Learned Publishing</i> 26 (1): 11–17. <a href="http://doi.org/10.1087/20130103" >10.1087/20130103</a>.
            </p>
            <div>
              <a href="references.html#adie2013">references</a>
            </div>
          </aside>
          <p>
            But, then again, this lack of meta-data suitable for classifying users is only a hinderance to the automated analysis and compilation of data.  Finding and flagging altmetric mentions in social-media discussion could still potentially provide usable information about the practical impact of research, if it could be followed by a “manual” analysis of the context of the discussion.  For example, Altmetric LLC provides for this process by using “data sources that can be manually audited by our users. If Altmetric says that an article has been tweeted about five times, then users should be able to get the relevant five links, Twitter usernames, and timestamps to go check for themselves” <a onclick="toggleAside('ref633');" >(Adie and Roe 2013)</a>. And if it were to a researcher's advantage to be able to demonstrate the practical impact of their research, it is would also be to that researcher's advantage to provide the resources needed to interpret the altmetric data related to their research.
          </p>
        </section>
      </span><span class="audience-p">&nbsp; <button class="toggle" onclick="toggleSpan('tog204');"> ± </button> &nbsp;</span></article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="dont-require-practical-impact-reward-it">
        <h2>
          Don't require practical impact; reward it
        </h2>
        <section>
          <p>
            If what has been lacking in research is practical impact, then isn't the easy solution to simply require all research to have some practical impact?  A form of this approach was proposed by Mitroff (1998), who posited that any research without practical impact was not <i>true</i> and therefore shouldn't be published.  To explain why practical impact might be a requirement for truth he wrote:
          </p>
          <blockquote>
            Pragmatism is the philosophical school that posits that truth is that which makes a significant difference in the lives of humans. Something&mdash;an action, empirical finding, proposition, conjecture, theorem—that is true in theory (i.e., in the abstract only) but makes no difference in the lives of humans is not a truth for pragmatism. Thus, contrary to what many academics believe, truth is not solely a property of formal propositions, theorems, research findings, and so forth but of ethical actions (i.e., actions that eliminate, or make significant headway in eliminating, some important human problem). <a onclick="toggleAside('ref661');">(Mitroff 1998)</a>
          </blockquote>
          <aside class="side-ref" id="ref661">
            <p>
              Mitroff, I. I. 1998. “On the Fundamental Importance of Ethical Management: Why Management Is the Most Important of All Human Activities.” <i>Journal of Management Inquiry</i> 7 (1): 68–79. <a href="http://doi.org/10.1177/105649269871011">10.1177/105649269871011</a>.
            </p>
            <div>
              <a href="references.html#mitroff1998">references</a>
            </div>
          </aside>
          <p>
            The problem with requiring practical impact for research is that it would block research into the fundamental laws and principles that underlie every field.  It would hamper research that attempts to synthesize general solutions to groups of related specific problems, since the general solution might “be broad and theoretical and… difficult to apply” <a onclick="toggleAside('ref672');" >(Hunlin 2001)</a>.
          </p>
          <aside class="side-ref" id="ref672">
            <p>
              Hulin, Charles. 2001. “Applied Psychology and Science: Differences between Research and Practice.” <i>Applied Psychology: An International Review</i> 50 (2): 225–34. <a href="http://doi.org/10.1111/1464-0597.00055">10.1111/1464-0597.00055</a>.
            </p>
            <div>
              <a href="references.html#hunlin2001">references</a>
            </div>
          </aside>
          <p>
            Instead of being required to produce research that has practical impact, researchers should be <em>rewarded</em> for the impact on practice that their research makes.  This makes practical impact important without making it so important that it hampers the free exploration that is vital to research.
          </p>
          <p>
            But, without at least some reward for practical impact, there is little incentive for researchers to spend time encouraging or nurturing the practical impact of their work.  Without the sorts of rewards that they get for presenting at academic conferences, or writing for academic journals, researchers won't spend their time and energy presenting at practitioner conferences, or writing for practitioner journals <a onclick="toggleAside('ref686');" >(Latham 2007)</a>.
          </p>
          <aside class="side-ref" id="ref686">
            <p>
              Latham, Gary P. 2007. “A Speculative Perspective on the Transfer of Behavioral Science Findings to the Workplace: ‘The Times They Are a-Changin’’.” <i>Academy of Management Journal</i> 50 (5): 1027–32. <a href="http://doi.org/10.5465/AMJ.2007.27153899" >10.5465/AMJ.2007.27153899</a>.
            </p>
            <div>
              <a href="references.html#latham2007">references</a>
            </div>
          </aside>
          <span class="t-text" id="tog449"><p>
            Rewarding researchers for having a positive impact on practice is not a new idea. While president of the Academy, Hambrick proposed that:
          </p>
          <blockquote>
            the Academy [of Management] should initiate a new major award: a prize for exceptional scholarly contribution to the practice of management. To be judged by a panel of distinguished academics and executives, this honor would be bestowed annually to an Academy member whose writings have had widespread beneficial impact on the arts and sciences of management, as practiced. <a onclick="toggleAside('ref700');" >(Hambrick 1994)</a>
          </blockquote>
          <aside class="side-ref" id="ref700">
            <p>
              Hambrick, Donald C. 1994. “What If the Academy Actually Mattered?” <i>The Academy of Management Review</i> 19 (1): 11–16. <a href="http://www.jstor.org/stable/258833" >http://www.jstor.org/stable/258833</a>.
            </p>
            <div>
              <a href="references.html#hambrick1994">references</a>
            </div>
          </aside></span><span class="audience-p">&nbsp; <button class="toggle" onclick="toggleSpan('tog449');"> ± </button> &nbsp;</span>
          <p>
            Rewarding practical impact can create the same benefits that would come from requiring it, without incurring the costs to the freedom of research that would come from actually requiring it.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="promote-practical-impact-and-trivial-results-will-fade">
        <h2>
          Promote practical impact, and trivial results will fade
        </h2>
        <p>
          This whole chain of reasoning started with the misapplication of NHST in research.  Would tracking practical impact, and rewarding researchers based on the practical impact of their research, correct that problem?
        </p>
        <p>
          It should, eventually.  Because the problematic sorts of research are never going to be lauded as having a positive effect on practice; they are too deeply buried in the minutiae of data for a practitioner to ever notice or care about them.
        </p>
        <p>
          But more importantly, this rather roundabout solution is a better solution than, say, creating a set of specific, statistical guidelines which are intended to prevent the trivial results from entering the literature.  <span class="t-text" id="tog475">Because that sort of solution is inherently flawed in way similar to one of the ways that the NHST is, “it provides the illusion of scientific rigour” <a onclick="toggleAside('ref730');">(Lockett <i>et al.</i> 2014)</a>. </span><span class="audience-p">&nbsp; <button class="toggle" onclick="toggleSpan('tog475');"> ± </button> &nbsp;</span> What is really required to solve the problems with the application of NHST is what <a onclick="toggleAside('ref738');" >Schwab <i>et al.</i> (2011)</a> called for: “Researchers should make thoughtful assessments”.
        </p>
          <aside class="side-ref" id="ref730">
            <p>
              Lockett, Andy, Abagail McWilliams, and David D. Van Fleet. 2014. “Reordering Our Priorities by Putting Phenomena before Design: Escaping the Straitjacket of Null Hypothesis Significance Testing.” <i>British Journal of Management</i> 25 (4): 863–73. <a href="http://doi.org/10.1111/1467-8551.12063">10.1111/1467-8551.12063</a>.
            </p>
            <div>
              <a href="references.html#lockett2014">references</a>
            </div>
          </aside>
          <aside class="side-ref" id="ref738">
            <p>
              Schwab, Andreas, Eric Abrahamson, William H. Starbuck, and Fiona Fidler. 2011. “PERSPECTIVE—Researchers Should Make Thoughtful Assessments Instead of Null-Hypothesis Significance Tests.” <i>Organization Science</i> 22 (4): 1105–20. <a href="http://doi.org/10.1287/orsc.1100.0557" >10.1287/orsc.1100.0557</a>.
            </p>
            <div>
              <a href="references.html#schwab2011">references</a>
            </div>
          </aside>
        <p>
          If we create a context where researchers are rewarded for thinking about the practical impact of their research, they will come up with solutions to the problems of trivial results and oversized data sets that are flexible and innovative and which will flourish on their own, with needing to be propped up by rules enforced by authorities.
        </p>
      </article>
    </main>
    <nav>
      <a href="index.html">
        <div>
          Abstract/Index
        </div>
      </a>
      <a href="practical-impact-thesis.html">
        <div>
          Thesis
        </div>
      </a>
      <a href="references.html">
        <div>
          References
        </div>
      </a>
    </nav>
    <footer></footer>
    <script>
      function toggleSpan (targetId) {
        var targetSpan = document.getElementById(targetId);
        targetSpan.style.display = ( targetSpan.style.display == 'inline' ? 'none' : 'inline' );
      };
      function toggleAside (targetId) {
        var targetSpan = document.getElementById(targetId);
        targetSpan.style.display = ( targetSpan.style.display == 'inline-block' ? 'none' : 'inline-block' );
      };
    </script>
  </body>
</html>
