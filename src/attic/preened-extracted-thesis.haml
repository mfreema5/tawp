  %h1
    Practical impact


  %article#what-is-practical-impact
    %h2
      What is “practical impact”?  It's when research informs practice.
    .lightbox
      %p
        Practitioners do things.  Researchers study how and why practitioners do the things they do.  Ideally, when researchers identify some previously unknown correlation between what practitioners do and the results they achieve, the practitioners can use that knowledge to improve their ability to get the result they want.
      %p
        This conversion of knowledge gained by researchers into a tool useable by practitioners is what I'm calling “practical impact”.


  %article#why-is-practical-impact-interesting
    %h2
      Why is the “practical impact” of research interesting?
    %section#trivial-correlations
      %h3
        On-going empirical research into trivial correlations in social sciences
      %p
        As evidenced by Schwab et al. (2011), in the social sciences it is often the case that merely demonstrating that an empirical study achieves statistical significant is considered the same as demonstrating that the study represents knowledge important enough to warrant being published.
      %p
        However, these days very large data sets are easy to find and even easier to analyze (when compared to earlier parts of the twentieth century, which is when most empirical methods were being developed and popularized), and such data have an inherently massive statistical power.  Meaning that they make it possible to separate even the tiniest of correlations between elements in that data from the statistical “noise” of synchronous yet random variations in the same data.
      %p
        Such inevitable correlations, while statistically significant, are often trivial when judged by any other measure.  [Schwab talks about…]
      %p
        What I propose is to judge these inevitable correlations by whether or not they become a tool for use by practitioners.  Especially in the social sciences, where controlled experiments are difficult if not impossible, the best confirmation of both the existence and the importance of a phenomenon is when the knowledge about that phenomenon is taken up by practitioners.
      %p
    %section
      %h3
        Value of practical impact may not be captured by citation metrics
      %p
        As funding agencies begin to track and use information of the practical value of research, research institutions themselves will need to start keeping an eye on the same thing.  Currently in academia, research is most often judged using various bibliometrics, all of which are tied to citations in academic journals.  But citation-based informetrics are almost entirely isolated to academia, and gather no information from practice or practitioners.
      %p
        Which means that “practical impact” is not being directly captured by citation-based metrics <a href="references.html#mohammadi2014">(Mohammadi <i>et al.</i> 2014)</a>.  Over the long term, adoption of a given piece of research knowledge will eventually circle back around and increase the citation metrics of the orignal publication, through the citations of researchers working on new studies who observe the knowledge being used in practice, and reference it.
      %p
        But that cycle will be slow and lossy, as it is dependent on researchers not only recognizing the use of knowledge from previous research, but also recognizing where it came from and citing that source when they publish their own work.
      %p
        So, citation-based metrics such as the Journal Impact Factor might reflect “practical impact”, but they do not capture it in a timely or accurate manner.


  %article#tracking-practical-impact
    %h2
      Tracking practical impact
    %section
      %h3
        Altmetrics breaks out of academic silo that isolates most bibliometric methods
      %p
        “Altmetrics” can refer to either the specific research-metric product from Altmetrics, LLC, or to any metric which uses data from outside of the databases of academic journals used by most, if not all, citation-based metrics.
      %p
        Both of those sorts of “altmetrics” have the potential to capture some of the practical impact of research, as they are not designed to exclusively use data from academia.  But they also don't exclude data from academia, either, which can obscure evidence of practical impact with the social chatter of academicians.
      %p
        Similarly, while methods for tracking the impact of research that use data passively provided by both researchers and practitioners (e.g., readership analysis), break out of the academia-only restriction that is inherent with a reliance on rigorous citations for evidence of impact, they also dilute the impact on practitioners in the flood of data from academics.
      %p
        An example is the study by Mohammadi et al. (2014) on Mendeley readership data.  As the authors explain, “Although the Mendeley API provides information related to the discipline, academic status and country of readers for each record, it only reports percentages rather than raw data and only gives information about the top three categories.”  So, if one of the top three categories of readers isn't a practitioner category, almost no information about an articles impact on practice is available.


  %article#conclusion-and-reiteration
    %h2
      Conclusion / Reiteration
    .lightbox
      %p
        Bibliometric methods are well established and widely used, but they reinforce the isolation of academic research from real-world practice.  That isolation has a number of consequences that include: the proliferation of trivial empirical studies; the failure of practitioners to benefit from the knowledge gains of researchers; and the increasingly skeptical scrutiny of the public funding of research.
      %p
        While requiring practical impact of academic research is both unlikely to be adopted and hazardous to the ongoing advancement of knowledge, the practical impact of research must be rewarded before researchers can be expected to look for and exploit opportunities for achieving practical impact in their research.
      %p
        Effective reward systems will require effective means of tracking the practical impact of research.  Current bibliometric and altmetric methodologies fail to capture practical impact in anything more than a glancing manner.
      %p
        An effective practical impact metric could draw data from a number of paths, including social media and readership information.  It would rank the practical impact of research with three non-quantitative levels: Unknown, Apparent and Important.  The distinction between the “A” and “I” levels would be based on comparisons within a field, to avoid disparities between fields that have inherently different potentials for practical impact, and established behaviors that might skew the tracking of practical impact.
