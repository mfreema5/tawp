<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Practical Impact - a thesis</title>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <link href="css/reset.css" media="screen,print" rel="stylesheet">
    <script src="css/modernizr.custom.99475.js"></script>
    <link href="css/screen.css" rel="stylesheet">
    <link href="css/gfx/favicon.ico" rel="icon" type="image/x-icon">
  </head>
  <body>
    <main>
      <h1>
        Practical impact
      </h1>
      <article id="what-is-practical-impact">
        <h2>
          What is “practical impact”?  It's when research informs practice.
        </h2>
        <div class="lightbox">
          <p>
            Practitioners do things.  Researchers study how and why practitioners do the things they do.  Ideally, when researchers identify some previously unknown correlation between what practitioners do and the results they achieve, the practitioners can use that knowledge to improve their ability to get the result they want.
          </p>
          <p>
            This conversion of knowledge gained by researchers into a tool useable by practitioners is what I'm calling “practical impact”.
          </p>
        </div>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="why-is-practical-impact-interesting">
        <h2>
          Why is the “practical impact” of research interesting?
        </h2>
        <section id="trivial-correlations">
          <h3>
            On-going empirical research into trivial correlations in social sciences
          </h3>
          <p>
            As evidenced by Schwab et al. (2011), in the social sciences it is often the case that merely demonstrating that an empirical study achieves statistical significant is considered the same as demonstrating that the study represents knowledge important enough to warrant being published.
          </p>
          <p>
            However, these days very large data sets are easy to find and even easier to analyze (when compared to earlier parts of the twentieth century, which is when most empirical methods were being developed and popularized), and such data have an inherently massive statistical power.  Meaning that they make it possible to separate even the tiniest of correlations between elements in that data from the statistical “noise” of synchronous yet random variations in the same data.
          </p>
          <p>
            Such inevitable correlations, while statistically significant, are often trivial when judged by any other measure.  [Schwab talks about…]
          </p>
          <p>
            What I propose is to judge these inevitable correlations by whether or not they become a tool for use by practitioners.  Especially in the social sciences, where controlled experiments are difficult if not impossible, the best confirmation of both the existence and the importance of a phenomenon is when the knowledge about that phenomenon is taken up by practitioners.
          </p>
          <p></p>
        </section>
        <section>
          <h3>
            Lack of relevance in management research
          </h3>
          <p class="lorem">
            In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a.
          </p>
        </section>
        <section>
          <h3>
            Increasing demand to demonstrate practical value of research by funding agencies
          </h3>
          <p>
            The HEFCE is intending to use the value of research to practitioners as part of its decision process for what research it will recommend gets ongoing government funding.  Other European institutions are doing similar things [pull refs from HEFCE-REF overview paper].
          </p>
        </section>
        <section>
          <h3>
            Value of practical impact may not be captured by citation metrics
          </h3>
          <p>
            As funding agencies begin to track and use information of the practical value of research, research institutions themselves will need to start keeping an eye on the same thing.  Currently in academia, research is most often judged using various bibliometrics, all of which are tied to citations in academic journals.  But citation-based informetrics are almost entirely isolated to academia, and gather no information from practice or practitioners.
          </p>
          <p>
            Which means that “practical impact” is not being directly captured by citation-based metrics (<a href="references.html#mohammadi2014">
              Mohammadi
              <i>et al.</i>
              2014
            </a>).  Over the long term, adoption of a given piece of research knowledge will eventually circle back around and increase the citation metrics of the orignal publication, through the citations of researchers working on new studies who observe the knowledge being used in practice, and reference it.
          </p>
          <p>
            But that cycle will be slow and lossy, as it is dependent on researchers not only recognizing the use of knowledge from previous research, but also recognizing where it came from and citing that source when they publish their own work.
          </p>
          <p>
            So, citation-based metrics such as the Journal Impact Factor might reflect “practical impact”, but they do not capture it in a timely or accurate manner.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="explaining-jif">
        <h2>
          What is a &ldquo;citation-based metrics such as the Journal Impact Factor&rdquo;?
        </h2>
        <section>
          <h3>
            Explaining the Journal Impact Factor through its historical foundations
          </h3>
          <p>
            To help explain what the Journal Impact Factor metric is&mdash;or, more exactly, what it does&mdash;we can trace its origins.  This history-based explanation draws heavily on: <a href="references.html#archambault2009">Archambault, Éric, and Vincent Larivière. 2009.</a> &ldquo;History of the Journal Impact Factor: Contingencies and Consequences.&rdquo; <i>Scientometrics</i> 79 (3): 635–49.
          </p>
        </section>
        <section>
          <h3>
            Finding the &ldquo;indispensable&rdquo; journals
          </h3>
          <p>
            Almost a hundred years ago, <a href="references.html#gross1927">Gross and Gross (1927)</a> posed the question:
          </p>
          <blockquote>
            What… scientific periodicals are needed in a college library successfully to prepare the student for advanced work, taking into consideration also those materials necessary for the stimulation and intellectual development of the faculty?
          </blockquote>
          <p>
            To answer that question of to which journals a library should subscribe, they used quantitative methods to make comparisons between journals.  Their methods eventually led to the Journal Impact Factor, and other related bibliometrics.
          </p>
          <p>
            Gross and Gross used a quantitive method to evaluate journals, as opposed to merely asking an expert to compile a list of &ldquo;indispensable&rdquo; journals in order to avoid, as they described it, a list that was &ldquo;seasoned too much by the needs, likes and dislikes of the compiler.&rdquo;  In other words, their goal was to achieve objectivity in the evaluation by using quantitative methods.
          </p>
        </section>
        <section>
          <h3>
            Counting citations to sort journals
          </h3>
          <p>
            The basic method pioneered by Gross and Gross, and repeated later by other researchers, was to select a pre-eminent journal or a reference that was generally accepted as key to a field, and then to compile and quantify the sources of the citations in that keystone reference/journal.
          </p>
          <p>
            An extremely simplified example: if the journal that was selected as the central reference for a field, &ldquo;Field&rdquo;, contained five citations to a &ldquo;Journal A&rdquo;, ten citations to a &ldquo;Journal B&rdquo;, and four citations to a &ldquo;Journal C&rdquo;, then for that field the journals would be ranked and reported something like this:
          </p>
          <ul>
          <li>Leading periodicals in Field
          
          <ul>
          <li>Journal B &ndash; 10</li>
          <li>Journal A &ndash; 5</li>
          <li>Journal C &ndash; 4</li>
          </ul></li>
          </ul>
          <p>
            These early citation metrics were inherently field-specific, since they took data from a central reference to some particular field.
          </p>
        </section>
        <section>
          <h3>
            Compiling citations for multiple fields
          </h3>
          <p>
            Soon, though researchers began to cross-compile citation information from multiple journals, and then to include journals from multiple fields in their compilations.  For exxample, <a href="references.html#gregory1937">Gregory (1937)</a> compiled more than 27,000 citations from across 27 subfields in medicine.
          </p>
          <p>
            Gregory gives a concise explanation of the purpose of her metrics:
          </p>
          <blockquote>
            The foregoing Tables (I-27) answer primarily the needs of the specialist in his attempt to keep adequately abreast of the literature in his field. Two further Tables have been compiled, to indicate to the medical library and the librarian (A), the indispensable periodicals for al the fields consulted; (B), a short list of essential periodicals in general medicine which cover a large amount of material. This latter list is designed primarily for the individual and for the small library.
          </blockquote>
          <p>
            There were 27 &ldquo;foregoing Tables&rdquo;, one for each of the subfields.  No comparisons were made between those fields.  This is a feature of similar metrics that would continue for decades: even when multiple fields were measured at the same time, the results for each field were reported separately.  The intended audience for these metrics had no need for cross-field comparisons.  Libraries are generally tasked to select the best journals for some particular field, and are not given carte blanche to choose which fields to support.  Similarly, researchers are interested in their field, not all fields.
          </p>
          <p>
            The compilation of citations from <a href="references.html#martyn1968" >Martyn and Gilchrist (1968)</a> is cited by <a href="references.html#archambault2009" >Archambault and Larivière (2009)</a> as having a large influence on methods used in the Journal Impact Factor, and it continues the practice of separating the reporting of metrics for separate fields.
          </p>
        </section>
        <section>
          <h3>
            Reporting ratios instead of counts
          </h3>
          <p>
            Another feature of the Martyn and Gilchrist compilation, was was carried forward into the Journal Impact Factor and other bibliometrics, was first proposed by <a href="references.html#hackh1936">Hackh (1936)</a>: the reporting of a ratio of the number of citations to the number of pieces that might be cited.
          </p>
          <p>
            Another extremely simplified example: if there were five citations to articles published in &ldquo;Journal A&rdquo; in the same year that &ldquo;Journal A&rdquo; published a total of 20 articles, the citation ratio would be &ldquo;5:20&rdquo;, or &ldquo;0.25&rdquo;.
          </p>
          <p>
            We can extend our example to include comparing journals.
          </p>
          <table>
            <thead>
              <tr>
                <th>Journal</th>
                <th>Citation count</th>
                <th>Cite-able pieces</th>
                <th>Ratio</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Journal A</td>
                <td>5</td>
                <td>20</td>
                <td>0.25</td>
              </tr>
              <tr>
                <td>Journal B</td>
                <td>10</td>
                <td>20</td>
                <td>0.50</td>
              </tr>
              <tr>
                <td>Journal C</td>
                <td>4</td>
                <td>10</td>
                <td>0.40</td>
              </tr>
            </tbody>
          </table>
          <ul>
          <li>Leading periodicals in Field
          
          <ul>
          <li>Journal B &ndash; 0.50</li>
          <li>Journal C &ndash; 0.40</li>
          <li>Journal A &ndash; 0.25</li>
          </ul></li>
          </ul>
          <p>
            This is the core of how the Journal Impact Factor and a number of similar metrics work.
          </p>
        </section>
        <section>
          <h3>
            Selecting what years to include
          </h3>
          <p>
            There is an important, but subtle, characteristic of compilations of citations left to be described, which has been carried forward all the way from the earliest forms to the Journal Impact Factor and other contemporary metrics.  But to explain it, we must first look at an important but obvious charactertisic: the time &ldquo;window&rdquo; within which citations must occur in order to be included in a compilation's counts.
          </p>
          <p>
            From the begining there was a time restriction around the citations included.  The <a href="references.html#gross1927">Gross and Gross (1927)</a> compilation only considered citations from 1926, because they drew all of their citations from the 1926 volume of <i>The Journal of the American Chemical Society</i>, which at the time was &ldquo;the most recent complete volume&rdquo;.  Other early metrics used restricted windows of time for similarly practical reasons.
          </p>
          <p>
            <a href="references.html#martyn1968">Martyn and Gilchrist (1968)</a> also give a practical reason for the restricted window of time in their citation metrics: &ldquo;the two-year sample reduced the effort of counting and also reduced the cost of acquisition of the data&rdquo;.  But they preface that with an interesting justification:
          </p>
          <blockquote>
            We decided that our most practical course would be to confine our study to citations made during 1965 to journals published in the two preceding years. It was already known that 26.1% of all the 1965 citations were to literature of 1964 and 1963, so in terms of number of citations this would give us an adequate sample.
          </blockquote>
          <p>
            Their method was not meant to be comprehensive.  In other words, it did not capture the entire population of citations, or pieces that could be cited.  Instead, the method operates on a sample of citations, and publications.
          </p>
        </section>
        <section>
          <h3>
            Samples, not populations, of citations
          </h3>
          <p>
            That these metrics are based on samples&mdash;as opposed to entire populations&mdash;is the important but non-obvious characteristic of citation compilations mentioned above.
          </p>
          <p>
            It's an important feature because many of the criticisms of the Journal Impact Factor can be viewed as simply issues related to the <em>sampling method</em> of the metric.  (What time window?  Which journals?  Which published pieces?)  And many of the contemporary bibliometric metrics that are offered as improvements to the Journal Impact Factor primarily differ from it only in the sampling methods employed.  Other than that they return the same citation-based analysis as the Journal Impact Factor.
          </p>
        </section>
        <section>
          <h3>
            So, what's a JIF, anyway?
          </h3>
          <p>
            In short: the Journal Impact Factor, and similar metrics:
          </p>
          <ul>
          <li>Sample citations in various journals</li>
          <li>Compare the number of actual citations to the number of potential citations</li>
          </ul>
          <p>
            The inherent (and unquestioned) assumption in all of this is that citation counts are strongly correlated with some attribute of research that is desirable.  It might be &ldquo;importance&rdquo;, or perhaps &ldquo;quality&rdquo;, or (inexactly) &ldquo;significance&rdquo;.  The nominal description of this attribute is &ldquo;Impact&rdquo;.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="tracking-practical-impact">
        <h2>
          Tracking practical impact
        </h2>
        <section>
          <h3>
            Altmetrics breaks out of academic silo that isolates most bibliometric methods
          </h3>
          <p>
            “Altmetrics” can refer to either the specific research-metric product from Altmetrics, LLC, or to any metric which uses data from outside of the databases of academic journals used by most, if not all, citation-based metrics.
          </p>
          <p>
            Both of those sorts of “altmetrics” have the potential to capture some of the practical impact of research, as they are not designed to exclusively use data from academia.  But they also don't exclude data from academia, either, which can obscure evidence of practical impact with the social chatter of academicians.
          </p>
          <p>
            Similarly, while methods for tracking the impact of research that use data passively provided by both researchers and practitioners (e.g., readership analysis), break out of the academia-only restriction that is inherent with a reliance on rigorous citations for evidence of impact, they also dilute the impact on practitioners in the flood of data from academics.
          </p>
          <p>
            An example is the study by Mohammadi et al. (2014) on Mendeley readership data.  As the authors explain, “Although the Mendeley API provides information related to the discipline, academic status and country of readers for each record, it only reports percentages rather than raw data and only gives information about the top three categories.”  So, if one of the top three categories of readers isn't a practitioner category, almost no information about an articles impact on practice is available.
          </p>
        </section>
        <section>
          <h3>
            The Centre for Science and Technology Studies (CWTS)
          </h3>
          <p class="lorem">
            Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.
          </p>
        </section>
        <section>
          <h3>
            Higher Education Funding Council for England &ndash; REF
          </h3>
          <p class="lorem">
            Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="uses-of-practical-impact-information">
        <h2>
          Uses of practical impact information
        </h2>
        <section>
          <h3>
            Determination of funding (e.g., HEFCE-REF)
          </h3>
          <p class="lorem">
            Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi.
          </p>
        </section>
        <section>
          <h3>
            Judging “quality of research”
          </h3>
          <p class="lorem">
            Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim.
          </p>
        </section>
        <section>
          <h3>
            Improving knowledge transfer between researchers and practitioners
          </h3>
          <p class="lorem">
            Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="hazards-of-ignoring-practical-value">
        <h2>
          Hazards of ignoring practical value
        </h2>
        <section>
          <h3>
            “Gaming” system through statistical power
          </h3>
          <p>
            <i>
              (See above:
              <a href="#trivial-correlations">“On-going empirical research into trivial correlations in social sciences”</a>).
            </i>
          </p>
          <p class="lorem">
            Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim.
          </p>
        </section>
        <section>
          <h3>
            Losing funding [see: HEFCE-REF]
          </h3>
          <p class="lorem">
            Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus.
          </p>
        </section>
        <section>
          <h3>
            Rewards that are based only citation-metrics begets research that maximizes citations, and only citations
          </h3>
          <p class="lorem">
            Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus.
          </p>
        </section>
        <section>
          <h3>
            Is it ethical to leave practitioners ignorant of knowledge that would improve their outcomes?
          </h3>
          <p class="lorem">
            Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="hazards-of-making-practical-value-requisite">
        <h2>
          Hazards of making practical value requisite
        </h2>
        <div class="lightbox">
          <p>
            If it is hazardous to ignore the practical impact of research, then should it not be required that all research have some practical impact?  A form of this requirement was proposed by
            <a href="references.html#mitroff1998">Mitroff (1998)</a>,
            who claimed that any research without practical impact was not
            <i>true</i>,
            and therefore shouldn't be published.  To explain why practical impact is a requirement for truth he wrote:
          </p>
          <blockquote cite="references.html#mitroff1998">
            <a class="nocolor" href="references.html#mitroff1998">Pragmatism is the philosophical school that posits that truth is that which makes a significant difference in the lives of humans. Something&mdash;an action, empirical finding, proposition, conjecture, theorem—that is true in theory (i.e., in the abstract only) but makes no difference in the lives of humans is not a truth for pragmatism. Thus, contrary to what many academics believe, truth is not solely a property of formal propositions, theorems, research findings, and so forth but of ethical actions (i.e., actions that eliminate, or make significant headway in eliminating, some important human problem).</a>
          </blockquote>
          <p>
            The most obvious problem with requiring practical impact for research is that it would block research into the fundamental laws and principles that underlie every field.  It would hamper research that attempted to synthesize general solutions to groups of related specific problems, since the general solution might “<q cite="references.html#hulin2001">be broad and theoretical and… difficult to apply</q>” (<a href="references.html#hulin2001">Hunlin 2001</a>).
          </p>
        </div>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="rewarding-practical-impact">
        <h2>
          Rewarding practical impact
        </h2>
        <div class="lightbox">
          <p>
            If there are hazards to requiring that research has practical impact, but also hazards to ignoring the practical impact of research, what should be done?  Researchers should be rewarded for the practical impact of their research.  This makes practical impact important, without making it so important that it hampers the free exploration that is vital to research.
          </p>
          <p>
            Without some sort of reward for practical impact, there is little incentive for researchers to spend time encouraging or nurturing the practical impact of their work.  Without the sorts of rewards that they get for presenting at academic conferences, or writing for academic journals, researchers won't spend their time and energy presenting at practitioner conferences, or writing for practitioner journals (<a href="references.html#latham2007">Latham 2007</a>).
          </p>
        </div>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="categorizing-practical-impact">
        <h2>
          Categorizing practical impact
        </h2>
        <section>
          <h3>
            Comparisons should always be within the same, or similar, fields
          </h3>
          <p>
            One of the more common criticisms of the Journal Impact Factor is that “<q cite="references.html#moed2010">it is improper to make comparisons between citation counts generated in different research fields, because the ‘citation potential’ can vary significantly from one field to another</q>” (<a href="references.html#moed2010">Moed 2010</a>).
          </p>
          <p>
            Similar to “citation potential”, the potential for practical impact varies between fields than does the “citation potential”.
            <a href="references.html#schwab2011">
              Schwab
              <i>et al.</i>
              (2011)
            </a>
            imply this disparity when they discuss why the social sciences continue to embrace null-hypothesis significant testing, while medical research has moved to replace it with methodologies that are, among other things, more predictive of practical impact.  The shift was drive by the fact that “Medical research… makes more of a difference to more people, and draws much more attention. Thus, medical researchers have greater incentive to measure and document effects of their work…”.  In other words, medical research has a much greater potential for practical impact than the social sciences.
          </p>
          <p>
            So, it would not be appropriate, for example, to measure the practical impact of research in the social science against a bar set by medical research.
          </p>
        </section>
        <section>
          <h3>
            Qualitative process
          </h3>
          <div class="subcontainer">
            <h4>
              Too many possible sources for practical impact data to equitably quantify
            </h4>
            <p>
              Altmetrics demonstrates one possible way to track practical impact.  It looks for (loosely defined) citations in social media data streams as a way to supplement citation-based metrics.  If the actors involved in those social media can be separated into classes of “academics” and “practitioners”, then the references to research made by members of the  practitioner class can become evidence of practical impact.
            </p>
            <p>
              Another existing example is analysis of data about the readership of research.
              <a href="references.html#mohammadi2014">
                Mohammadi
                <i>et al.</i>
                (2014)
              </a>
              looked at the readership data of Mendeley, where users self-identify their profession, so that the Mendely API can distinguish between academic and non-academic readers.  Being read by non-academic users can be evidence of the practical impact of a research article.  Though, there are some problems with trying to use the Mendeley API this way, since “<q cite="references.html#mohammadi2014">the Mendeley API provides information related to the discipline, academic status and country of readers for each record, [but] it only reports percentages rather than raw data and only gives information about the top three categories</q>” (<a href="references.html#mohammadi2014">
                <i>ibid</i>
              </a>).
            </p>
            <p>
              But, following the logic of Mendeley-readership analysis, any computer-mediated reading process becomes a potential path for identifying practical impact.
            </p>
            <p>
              However, not all of these possible paths are going to present the same strength of evidence of practical impact.  The nature of the evidence&mdash;e.g., the reading of an article as opposed to actively tweeting about it&mdash;will effect its strength, as will the particular path from which it comes&mdash;e.g., readership data from a source that only reports the top-three readership classes versus one that reports the top ten.
            </p>
            <p>
              So doing quantitative transformations and comparisons with the evidence of practical impact being proposed here is simply not reasonable, or defensible.
            </p>
          </div>
          <div class="subcontainer">
            <h4>
              Different fields have varying levels of practical impact
            </h4>
            <p>
              Another impediment to the quantification of practical impact evidence is that different fields will have different levels of practical impact in general.  So, a research paper that has an important level of practical impact in one field would only qualify as having an apparent level of practical impact in another.  Researchers working in more esoteric fields should not be measured against bars set by researchers in fields like medical research which are very closely tied to practice.
            </p>
          </div>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="levels-of-practical-impact">
        <h2>
          Levels of practical impact
        </h2>
        <section>
          <h4>
            U – Unknown/undefined
          </h4>
          <p>
            No examples of practical impact
          </p>
        </section>
        <section>
          <h4>
            A – Apparent
          </h4>
          <p>
            At least one example of practical impact
          </p>
        </section>
        <section>
          <h4>
            I – Important
          </h4>
          <p>
            Multiple examples of practical impact; has more impact than other research in the same field (with at least an A-level)
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article id="conclusion-and-reiteration">
        <h2>
          Conclusion / Reiteration
        </h2>
        <div class="lightbox">
          <p>
            Bibliometric methods are well established and widely used, but they reinforce the isolation of academic research from real-world practice.  That isolation has a number of consequences that include: the proliferation of trivial empirical studies; the failure of practitioners to benefit from the knowledge gains of researchers; and the increasingly skeptical scrutiny of the public funding of research.
          </p>
          <p>
            While requiring practical impact of academic research is both unlikely to be adopted and hazardous to the ongoing advancement of knowledge, the practical impact of research must be rewarded before researchers can be expected to look for and exploit opportunities for achieving practical impact in their research.
          </p>
          <p>
            Effective reward systems will require effective means of tracking the practical impact of research.  Current bibliometric and altmetric methodologies fail to capture practical impact in anything more than a glancing manner.
          </p>
          <p>
            An effective practical impact metric could draw data from a number of paths, including social media and readership information.  It would rank the practical impact of research with three non-quantitative levels: Unknown, Apparent and Important.  The distinction between the “A” and “I” levels would be based on comparisons within a field, to avoid disparities between fields that have inherently different potentials for practical impact, and established behaviors that might skew the tracking of practical impact.
          </p>
        </div>
      </article>
    </main>
    <nav>
      <a href="index.html"></a>
      <div>
        Abstract/Index
      </div>
      <a href="practical-impact-thesis.html"></a>
      <div>
        Thesis
      </div>
      <a href="references.html"></a>
      <div>
        References
      </div>
    </nav>
    <footer></footer>
  </body>
</html>
