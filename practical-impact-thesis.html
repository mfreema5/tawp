<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Practical Impact - a thesis</title>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <link href="css/reset.css" media="screen,print" rel="stylesheet">
    <script src="css/modernizr.custom.99475.js"></script>
    <link href="css/screen.css" rel="stylesheet">
    <link href="css/gfx/favicon.ico" rel="icon" type="image/x-icon">
  </head>
  <body>
    <main>
      <h1>
        Practical impact &ndash; a thesis
      </h1>
      <article>
        <h2>
          &nbsp;
        </h2>
        <section>
          <h3>
            Preface: document structure
          </h3>
          <p>
            The structure of this thesis roughly maps to the chain of reasoning that I followed when considering the issues that led to and arise from the notion of tracking the “practical impact” of research.  I started with the problem of the common misapplication of statistical methods in research in management studies.  That problem, however, is really a just one part of the more general issue of research that lacks relevance outside of academia, not only in the management literature, but in other social science as well.
          </p>
          <p>
            Since there are already processes in place to identify the value of research to other researchers&mdash;citation metrics&mdash;I wondered if similar processes couldn't be developed to identify the value of research to practitioners.  Such a process would need to involve two parts: 1 - finding appropriate information, and 2 - applying that information in an appropriate manner.  I examined the existing metrics for measuring the “scientific” (as opposed to “practical”) value of research to see what lessons could be learned about gathering information for a future practical impact metric.  I also examined how current metrics are used, in addition to researching previous proposals (and subsequent discussions) about including practice in the evaluation of research, to gain insights into how to best apply the output of a practical impact metric.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article>
        <h2>
          Null-hypothesis significance testing
        </h2>
        <section>
          <h3>
            Losing importance in the search for significance
          </h3>
          <p>
            Null-hypothesis significance testing (NHST) is a well-established and widespread statistical technique.  It allows a researcher to calculate the probability that variables which appear to be correlated actually are correlated.  When there is a high probability that a correlation is real&mdash;as opposed to the result of random chance&mdash;that correlation is given the rather unfortunately-chosen description of “statistically significant.”
          </p>
          <p>
            It's an unfortunate label because it's easy shortened to “significant”, and mistaken for a judgement on the importance of a empirical results, as opposed to being only a statement about the chances of it being in error.
          </p>
          <p>
            Researchers can unconsciously exploit this confusion to their advantage by focusing on research that results in high probabilities of statistical significance and is therefore likely to be published, and avoiding research that is potentially of great importance, but that doesn't have a high probability of statistical significance.
          </p>
          <p>
            For example, in an editorial in the <i>Academy of Management Journal</i>, Combs questioned whether the ever-more-easy availability of large data sets couple with the ease of performing NHST afford by modern computers and software combine to:
          </p>
          <blockquote>
            …mask other shortcomings of our research designs and leave us with itty-bitty effect sizes that limit the relevance of our research. …As management scholars, can we really suggest that managers should change their decision calculus on the basis of knowledge that some new variable explains .0025 percent of the variance in organizational performance? <a href="references.html#combs2010">(Combs 2010)</a>
          </blockquote>
          <p>
            Combs went beyond merely speculating on a shift toward larger data sets with smaller effects, and did some comparisons between studies published in the 1987-89 volumes and in the 2007-8 volumes of the <i>Academy of Management Journal</i>.  During that 20-year period, the mean sample size rose from 300 to 3,423 (excluding the three largest samples from the latter period that, if included, would raise the latter mean to 7,578).  Meanwhile, the mean correlations fell from .22, to .17.  The larger samples made smaller and smaller correlations statistically significant <a href="references.html#combs2010">(Combs 2010)</a>.
          </p>
          <p>
            Ellis did a similar survey of empirical studies in the <i>Journal of International Business Studies</i> and found a similar shift toward larger sample sizes, with a similar detrimental effect on relevance of results:
          </p>
          <blockquote>
            In many of the studies I read it was apparent that researchers confused statistical with substantive significance when interpreting their results. This usually happened when conclusions about effects were drawn solely by looking at the p-values of test results. The problem with this is that the p-values generated by statistical significance tests are confounded indices that reflect both the size of the effect as it occurs in the population and the sample size used to detect it. As N goes up p goes down, irrespective of the underlying effect size. The implication is that trivial results are sometimes interpreted as meaningful in large-N studies…. <a href="references.html#ellis2010">(Ellis 2010)</a>
          </blockquote>
          <p>
            <a href="references.html#lockett2014">Lockett <i>et al.</i> (2014)</a> note that the problem of misapplying NHST is not limited to managent research: “Commentators have detailed the problems of employing NHST across a range of different social science disciplines including management, economics and psychology". And, more so, the concerns are not even restricted to the social sciences, as their are “similar concerns in medicine, natural sciences and engineering”.
          </p>
          <p>
            Lockett, <i>et al.</i>, also point out that NHST has become central to the question of whether or not a paper that presents an empirical study will be published since statistical significance is often (incorrectly) used a proxy for importance. As they put it: NHST “is commonly employed as the sole criterion in deciding on whether or not an effect size is (statistically) significant, with no real consideration of whether or not the effect size is substantively significant (i.e. it really matters)” <a href="references.html#lockett2014">(Lockett <i>et al.</i> 2014)</a>.
          </p>
          <p>
            They then close the loop, mentioning that since: A - larger sample sizes make even trivial effects statistically significant; and B - statistical significance is central to whether or not a paper gets published; then it follows that “generating a large sample size is an important factor in determining the probability of a paper being published in a top-rated journal”.  This is in spite the fact that “…the impact of the research may be significantly undermined because the results that are deemed important under the criterion of statistical significance may well be trivial in terms of substantive significance” <a href="references.html#lockett2014">(Lockett <i>et al.</i> 2014)</a>.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article>
        <h2>
          Making importance once again important
        </h2>
        <section>
          <h3>
            Demonstrate importance through effect sizes
          </h3>
          <p>
            A corrective action to limit the erosion of the importance of research due to the misapplication of NHST was summarized nicely by Schwab <i>et al.</i> when they wrote that “researchers should show the substantive importance of findings by reporting effect sizes” <a href="references.html#schwab2011" >(Schwab <i>et al.</i> 2011)</a>.
          </p>
          <p>
            Using effects size to supplant the importance afforded to NHST comes up repeatedly in the review by <a href="references.html#fidler2004" >Fidler <i>et al.</i> (2004)</a> of various statistical reforms in medicine, psychology and other fields.  For example, they report that the guidelines from an APA Task Force on Statistical Inference for publishing statistical results “strongly emphasised the need for measures of effect size to be the primary outcome of a study, and the need to differentiate clinical or practical significance from statistical significance”.
          </p>
        </section>
        <section>
          <h3>
            From calculations to precedents
          </h3>
          <p>
            However, while moving from relying on statistical significance alone to instead using statistical significance in combination with determinations of the effect size is conceptually easy, in practice it can be a major change.
          </p>
          <p>
            For example, consider the complications that Fidler <i>et al.</i> detail to explain why the move to effect sizes that was an effective method of statistical reform in medical research, failed to take hold in psychology:
          </p>
          <blockquote>
            One often-suggested solution is to standardise the effect sizes, that is, report the effect in units of standard deviation. However, it is important to recognise that CIs for standardized measures are not straightforward. They typically require non-central distributions, something most psychologists have never encountered. Also, they involve highly intensive computing processes that have only recently been accessible to individual researchers.<br /><br />These are not problems medicine encountered. The conceptual shift to estimation was relatively straightforward and so were CI calculations for what became the effect sizes of choice, odds ratios and relative risk values. <a href="references.html#fidler2004">(Fidler <i>et al.</i> 2004)</a>
          </blockquote>
        </section>
        <section>
          <h3>
            Using precedents as estimates
          </h3>
          <p>
            However, there may be something of a shortcut.  What Fidler <i>et al.</i> are describing is a mathematically rigorous determination of the effect size of a research result.
          </p>
          <p>
            But is that what is needed?  The goal is simply to ensure that the size of the effect being reported in a study isn't irrelevant.  Or, to come at it from the other direction, what is needed is some estimation that the effect size is sufficiently large enough to be relevant to practitioners.
          </p>
          <p>
            We can possibly employ the same approach that is used when designing a statistical study.  Determining of the statistical power requirements of a study requires that you know the effect size, yet the effect size is typically one of the things you'll be measuring with the study.  Therefore, what is commonly done is to estimate the effect size, based on previous research.
          </p>
          <p>
            So, we can estimate what sort of effect size will be of interest to practitioners by looking at what effect sizes they have previously found interesting.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article>
        <h2>
          Simplify the problem…
        </h2>
        <section>
          <h3>
            …By expanding it
          </h3>
          <p>
            Up to this point the type of research in question has been rather narrow: empirical studies that employ NHST.
          </p>
          <p>
            But that is not the only sort of research that might be relevant to practitioners.  More importantly, that is not the only type of research that has the potential to be relevant to practitioners, but is failing to fullfil that potential.
          </p>
          <p>
            For example, <a href="references.html#rynes2001">Rynes <i>et al.</i> (2001)</a> were not speaking only of empirical studies that use NHST when they wrote that, “A substantial body of evidence suggests that executives typically do not turn to academics or academic research findings in developing management strategies and practices”.
          </p>
          <p>
            Also, as part of his incoming Presidential Address to the Academy of Management—which had the unsubtle title of “What If the Academy Actually Mattered?”—<a href="references.html#hambrick1994">Hambrick (1993)</a> wrote about the general gap between researchers and practitioners in that field:
          </p>
          <blockquote>
            Rather, we simply seem to have a minimalist ethos: minimal programming, minimal dues, minimal staff, minimal innovation, minimal visibility, minimal impact. Each August, we come to talk with each other; during the rest of the year we read each others' papers in our journals and write our own papers so that we may, in turn, have an audience the following August: an incestuous, closed loop.
          </blockquote>
          <p>
            There would seem to be no reason to we must restrict ourselves to asking “What effect sizes, in which correlations, would a practitioner care about?”  The failure to achieve relevance is more general than that, so we can ask a more general question: “What research would a practitioner care about?”
          </p>
        </section>
        <section>
          <h3>
            Find out from practitioners what they consider relevant
          </h3>
          <p>
            The obvious way to get an answer to the more general question proposed in the previous section would be to pose it directly to a practitioner: “What research do you care about?"
          </p>
          <p>
            It turns out that there is a precedent this sort of question.  Almost one hundred years ago, Gross and Gross posed the question:
          </p>
          <blockquote>
            What… scientific periodicals are needed in a college library successfully to prepare the student for advanced work, taking into consideration also those materials necessary for the stimulation and intellectual development of the faculty? <a href="references.html#gross1927">(Gross and Gross 1927)</a>
          </blockquote>
          <p>
            What makes it similar to our question about what research would be of interest to practitioners is that “one way to answer this question would be merely to sit down and compile a list of those journals which one considers indispensable” <a href="references.html#gross1927">(Gross and Gross, 1927)</a>.  In other words, the first way that comes to mind to address the question is to go and ask the subject of the question.
          </p>
          <p>
            However, as <a href="references.html#gross1927">Gross and Gross (1927)</a> go on to explain, while you might get good results from a direct inquiry, “it seems reasonably certain that often the result would be seasoned too much by the needs, likes and dislikes of the compiler.”
          </p>
          <p>
            What they ended up doing instead of directly asking the subject, was to collect data on the subject's behavior, and extrapolate from that what journals the subject finds “indispensable”.  Specifically, they looked at citations in and about journals; and started the chain of research that led to the Journal Impact Factor and all the myriad other citation-based metrics of research.
          </p>
          <p>
            Following that example, in order to discover what research practitioners care about, we should find out what research is being used by practitioners, and what research are they discussing among themselves.
          </p>
        </section>
        <section>
          <h3>
            Specifics of what would be tracked
          </h3>
          <p>
            When practitioners use research results in their practice, or discuss them with other practitioners, we'll say that the research had a “practical impact”, to distinguish from “scientific impact” or “citation impact”.
          </p>
          <p>
            Studies in the area of altmetrics have demonstrated the viability of tracking at least some of the research-relevance-indicating behaviors of practitioners, along with tracking what <a href="references.html#adie2013">Adie and Roe (2013)</a> called “mentions”, that is, links to research documents in main-stream media sources, and social media discussions. For specific examples, see the section below on “Altmetrics.”
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article>
        <h2>
          Existing metrics
        </h2>
        <p class="lightbox">
          There exists a variety of metrics for judging the scientific quality research.  How those metrics are constructed and used can should inform any metric intended to track the practical impact of research.
        </p>
        <section>
          <h3>
            Journal-level citation metrics
          </h3>
          <p>
            All citation-based metrics are typically traced back to the paper by Gross and Gross (1927) mentioned above.  <a href="references.html#archambault2009">Archambault and Larivière (2009)</a> tap it as the starting point for the Journal Impact Factor, the best known and possibly most widely used citation-based metric for research.
          </p>
          <p>
            As an example of citation-based metrics in general, I'll trace the origins of the Journal Impact Factor, drawing heavily on &lsquo;Archambault, Éric, and Vincent Larivière. 2009. “History of the Journal Impact Factor: Contingencies and Consequences.” <i>Scientometrics</i> 79 (3): 635–49. <a href="http://doi.org/10.1007/s11192-007-2036-x" >10.1007/s11192-007-2036-x</a>&rsquo;.
          </p>
        </section>
        <section>
          <h3>
            Finding the &ldquo;indispensable&rdquo; journals
          </h3>
          <p>
            In 1927, as mentioned above, <a href="references.html#gross1927">Gross and Gross (1927)</a> posed the question, “What… scientific periodicals are needed in a college library?”
          </p>
          <p>
            To answer that question of to which journals a library should subscribe, they used quantitative methods to make comparisons between journals.  Their methods eventually led to the Journal Impact Factor, and other related bibliometrics.
          </p>
          <p>
            Gross and Gross used a quantitive method to evaluate journals, as opposed to merely asking an expert to compile a list of &ldquo;indispensable&rdquo; journals in order to avoid, as they described it, a list that was &ldquo;seasoned too much by the needs, likes and dislikes of the compiler.&rdquo;  In other words, their goal was to achieve objectivity in the evaluation by using quantitative methods.
          </p>
        </section>
        <section>
          <h3>
            Counting citations to sort journals
          </h3>
          <p>
            The basic method pioneered by Gross and Gross, and repeated later by other researchers, was to select a pre-eminent journal or a reference that was generally accepted as key to a field, and then to compile and quantify the sources of the citations in that keystone reference/journal.
          </p>
          <p>
            An extremely simplified example: if the journal that was selected as the central reference for a field, &ldquo;Field&rdquo;, contained five citations to a &ldquo;Journal A&rdquo;, ten citations to a &ldquo;Journal B&rdquo;, and four citations to a &ldquo;Journal C&rdquo;, then for that field the journals would be ranked and reported something like this:
          </p>
          <ul>
          <li>Leading periodicals in Field
          
          <ul>
          <li>Journal B &ndash; 10</li>
          <li>Journal A &ndash; 5</li>
          <li>Journal C &ndash; 4</li>
          </ul></li>
          </ul>
          <p>
            These early citation metrics were inherently field-specific, since they took data from a central reference to some particular field.
          </p>
        </section>
        <section>
          <h3>
            Compiling citations for multiple fields
          </h3>
          <p>
            Soon, though researchers began to cross-compile citation information from multiple journals, and then to include journals from multiple fields in their compilations.  For exxample, <a href="references.html#gregory1937">Gregory (1937)</a> compiled more than 27,000 citations from across 27 subfields in medicine.
          </p>
          <p>
            Gregory gives a concise explanation of the purpose of her metrics:
          </p>
          <blockquote>
            The foregoing Tables (I-27) answer primarily the needs of the specialist in his attempt to keep adequately abreast of the literature in his field. Two further Tables have been compiled, to indicate to the medical library and the librarian (A), the indispensable periodicals for al the fields consulted; (B), a short list of essential periodicals in general medicine which cover a large amount of material. This latter list is designed primarily for the individual and for the small library.
          </blockquote>
          <p>
            There were 27 &ldquo;foregoing Tables&rdquo;, one for each of the subfields.  No comparisons were made between those fields.  This is a feature of similar metrics that would continue for decades: even when multiple fields were measured at the same time, the results for each field were reported separately.  The intended audience for these metrics had no need for cross-field comparisons.  Libraries are generally tasked to select the best journals for some particular field, and are not given carte blanche to choose which fields to support.  Similarly, researchers are interested in their field, not all fields.
          </p>
          <p>
            The compilation of citations from <a href="references.html#martyn1968" >Martyn and Gilchrist (1968)</a> is cited by <a href="references.html#archambault2009" >Archambault and Larivière (2009)</a> as having a large influence on methods used in the Journal Impact Factor, and it continues the practice of separating the reporting of metrics for separate fields.
          </p>
        </section>
        <section>
          <h3>
            Reporting ratios instead of counts
          </h3>
          <p>
            Another feature of the Martyn and Gilchrist compilation, was was carried forward into the Journal Impact Factor and other bibliometrics, was first proposed by <a href="references.html#hackh1936">Hackh (1936)</a>: the reporting of a ratio of the number of citations to the number of pieces that might be cited.
          </p>
          <p>
            Another extremely simplified example: if there were five citations to articles published in &ldquo;Journal A&rdquo; in the same year that &ldquo;Journal A&rdquo; published a total of 20 articles, the citation ratio would be &ldquo;5:20&rdquo;, or &ldquo;0.25&rdquo;.
          </p>
          <p>
            We can extend our example to include comparing journals.
          </p>
          <table>
            <thead>
              <tr>
                <th>Journal</th>
                <th>Citation count</th>
                <th>Cite-able pieces</th>
                <th>Ratio</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Journal A</td>
                <td>5</td>
                <td>20</td>
                <td>0.25</td>
              </tr>
              <tr>
                <td>Journal B</td>
                <td>10</td>
                <td>20</td>
                <td>0.50</td>
              </tr>
              <tr>
                <td>Journal C</td>
                <td>4</td>
                <td>10</td>
                <td>0.40</td>
              </tr>
            </tbody>
          </table>
          <ul>
          <li>Leading periodicals in Field
          
          <ul>
          <li>Journal B &ndash; 0.50</li>
          <li>Journal C &ndash; 0.40</li>
          <li>Journal A &ndash; 0.25</li>
          </ul></li>
          </ul>
          <p>
            This is the core of how the Journal Impact Factor and a number of similar metrics work.
          </p>
        </section>
        <section>
          <h3>
            Selecting what years to include
          </h3>
          <p>
            There is an important, but subtle, characteristic of compilations of citations left to be described, which has been carried forward all the way from the earliest forms to the Journal Impact Factor and other contemporary metrics.  But to explain it, we must first look at an important but obvious charactertisic: the time &ldquo;window&rdquo; within which citations must occur in order to be included in a compilation's counts.
          </p>
          <p>
            From the begining there was a time restriction around the citations included.  The <a href="references.html#gross1927">Gross and Gross (1927)</a> compilation only considered citations from 1926, because they drew all of their citations from the 1926 volume of <i>The Journal of the American Chemical Society</i>, which at the time was &ldquo;the most recent complete volume&rdquo;.  Other early metrics used restricted windows of time for similarly practical reasons.
          </p>
          <p>
            <a href="references.html#martyn1968">Martyn and Gilchrist (1968)</a> also give a practical reason for the restricted window of time in their citation metrics: &ldquo;the two-year sample reduced the effort of counting and also reduced the cost of acquisition of the data&rdquo;.  But they preface that with an interesting justification:
          </p>
          <blockquote>
            We decided that our most practical course would be to confine our study to citations made during 1965 to journals published in the two preceding years. It was already known that 26.1% of all the 1965 citations were to literature of 1964 and 1963, so in terms of number of citations this would give us an adequate sample.
          </blockquote>
          <p>
            Their method was not meant to be comprehensive.  In other words, it did not capture the entire population of citations, or pieces that could be cited.  Instead, the method operates on a sample of citations, and publications.
          </p>
        </section>
        <section>
          <h3>
            Samples, not populations, of citations
          </h3>
          <p>
            That these metrics are based on samples&mdash;as opposed to entire populations&mdash;is the important but non-obvious characteristic of citation compilations mentioned above.
          </p>
          <p>
            It's an important feature because many of the criticisms of the Journal Impact Factor can be viewed as simply issues related to the <em>sampling method</em> of the metric.  (What time window?  Which journals?  Which published pieces?)  And many of the contemporary bibliometric metrics that are offered as improvements to the Journal Impact Factor primarily differ from it only in the sampling methods employed.  Other than that they return the same citation-based analysis as the Journal Impact Factor.
          </p>
        </section>
        <section>
          <h3>
            So, what's a JIF, anyway?
          </h3>
          <p>
            In short: the Journal Impact Factor, and similar metrics:
          </p>
          <ul>
          <li>Sample citations in various journals</li>
          <li>Compare the number of actual citations to the number of potential citations</li>
          </ul>
        </section>
        <section>
          <h3>
            Altmetrics - overview
          </h3>
          <p>
            According to <a href="references.html#zahedi2014">Zahedi <i>et al.</i> (2014)</a> alternative metrics include, “usage data analysis (download and view counts); web citation, and link analyses.”  An even more general description is that altmetrics are intended to use data available on the web to supplement and improve upon citation-based metrics for measuring the impact of science and research.  Through the access to information made available to the web, altmetrics can go beyond the journal articles and books included in citation-based metrics to include “other outputs such as datasets, software, slides, blog posts, etc.” <a href="references.html#zahedi2014">(Zahedi <i>et al.</i> 2014)</a>.
          </p>
          <p>
            Instead of compiling citations to research in journals, altmetrics involves compiling “mentions” of research in “main-stream media sources, and social media shares and discussions”, along with statistics on downloads, and reference manager counts (<a href="references.html#adie2013">Adie and Roe 2013)</a>.
          </p>
          <p>
            Altmetrics also typically retain the related meta-data of mentions and usage statistics, which allow for more complex analyses of the information.  For example, <a href="references.html#mohammadi2014">Mohammadi et al. (2014)</a> studied the relationship between citation metrics and the usage statistics and user information available from a reference manager service, and found some insights into the the limitations of citation-based metrics.
          </p>
          <p>
            So, altmetrics not only track what research is being “mentioned”, but also where it is mentioned, and who is mentioning it.
          </p>
        </section>
        <section>
          <h3>
            Altmetrics - examples related to practical impact
          </h3>
          <p class="lorem">
            Bacon ipsum dolor amet ex salami sunt flank et short loin enim shankle. Magna commodo kevin meatball tail drumstick ullamco, dolor alcatra chicken ham venison ham hock sausage beef ribs. Ribeye dolore ham occaecat commodo, duis sunt biltong turkey cupim. Sausage minim tenderloin, ipsum cupim nostrud dolore flank. T-bone eiusmod officia, ball tip tenderloin frankfurter short loin meatball incididunt nulla irure. Fatback prosciutto deserunt, reprehenderit kielbasa eiusmod rump bacon.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article>
        <h2>
          Don't require practical impact; reward it
        </h2>
        <section>
          <p>
            If what has been lacking in research is practical impact, then isn't the easy solution to simply require all research to have some practical impact?  A form of this approach was proposed by Mitroff (1998), who posited that any research without practical impact was not <i>true</i> and therefore shouldn't be published.  To explain why practical impact might be a requirement for truth he wrote:
          </p>
          <blockquote>
            Pragmatism is the philosophical school that posits that truth is that which makes a significant difference in the lives of humans. Something&mdash;an action, empirical finding, proposition, conjecture, theorem—that is true in theory (i.e., in the abstract only) but makes no difference in the lives of humans is not a truth for pragmatism. Thus, contrary to what many academics believe, truth is not solely a property of formal propositions, theorems, research findings, and so forth but of ethical actions (i.e., actions that eliminate, or make significant headway in eliminating, some important human problem). <a href="references.html#mitroff1998">(Mitroff 1998)</a>
          </blockquote>
          <p>
            The problem with requiring practical impact for research is that it would block research into the fundamental laws and principles that underlie every field.  It would hamper research that attempts to synthesize general solutions to groups of related specific problems, since the general solution might “be broad and theoretical and… difficult to apply” <a href="references.html#hulin2001" >(Hunlin 2001)</a>.
          </p>
          <p>
            Instead of being required to produce research that has practical impact, researchers should be <em>rewarded</em> for the impact on practice that their research makes.  This makes practical impact important without making it so important that it hampers the free exploration that is vital to research.
          </p>
          <p>
            But, without at least some reward for practical impact, there is little incentive for researchers to spend time encouraging or nurturing the practical impact of their work.  Without the sorts of rewards that they get for presenting at academic conferences, or writing for academic journals, researchers won't spend their time and energy presenting at practitioner conferences, or writing for practitioner journals <a href="references.html#latham2007">(Latham 2007)</a>.
          </p>
          <p>
            Rewarding researchers for having a positive impact on practice is not a new idea. While president of the Academy, Hambrick proposed that:
          </p>
          <blockquote>
            the Academy [of Management] should initiate a new major award: a prize for exceptional scholarly contribution to the practice of management. To be judged by a panel of distinguished academics and executives, this honor would be bestowed annually to an Academy member whose writings have had widespread beneficial impact on the arts and sciences of management, as practiced. <a href="references.html#hambrick1994" >(Hambrick 1994)</a>
          </blockquote>
          <p>
            Rewarding practical impact can create the same benefits that would come from requiring it, without incurring the costs to the freedom of research that would come from actually requiring it.
          </p>
        </section>
      </article>
      <nav>
        <a href='index.html'><div>Index</div></a>
        <a href='references.html'><div>Refs</div></a>
      </nav>
      <article>
        <h2>
          Promote practical impact, and trivial results will fade
        </h2>
        <p>
          This whole chain of reasoning started with the misapplication of NHST in research.  Would tracking practical impact, and rewarding researchers based on the practical impact of their research, correct that problem?
        </p>
        <p>
          It should, eventually.  Because the problematic sorts of research are never going to be lauded as having a positive effect on practice; they are too deeply buried in the minutiae of data for a practitioner to ever notice or care about them.
        </p>
        <p>
          But more importantly, this rather roundabout solution is a better solution than, say, creating a set of specific, statistical guidelines which are intended to prevent the trivial results from entering the literature.  Because that sort of solution is inherently flawed in way similar to one of the ways that the NHST is, “it provides the illusion of scientific rigour” <a href="references.html#lockett2014">(Lockett <i>et al.</i> 2014)</a>.  What is really required to solve the problems with the application of NHST is what <a href="references.html#schwab2011">Schwab <i>et al.</i> (2011)</a> called for: “Researchers should make thoughtful assessments”.
        </p>
        <p>
          If we create a context where researchers are rewarded for thinking about the practical impact of their research, they will come up with solutions to the problems of trivial results and oversized data sets that are flexible and innovative and which will flourish on their own, with needing to be propped up by rules enforced by authorities.
        </p>
      </article>
    </main>
    <nav>
      <a href="index.html">
        <div>
          Abstract/Index
        </div>
      </a>
      <a href="practical-impact-thesis.html">
        <div>
          Thesis
        </div>
      </a>
      <a href="references.html">
        <div>
          References
        </div>
      </a>
    </nav>
    <footer></footer>
  </body>
</html>
