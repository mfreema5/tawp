<h4 id="refinements-on-the-journal-impact-factor">Refinements on the Journal Impact Factor</h4>
<h4 id="snip-source-normalized-indicator-of-journal-impact-per-paper"><em>SNIP</em> – Source Normalized Indicator of journal impact per Paper</h4>
<p>As mentioned earlier, the lack of cross-field comparisons in the Journal Impact Factor means that its use is problematic for institutions that want to equitably evaluate the quality of research done by faculty and staff in an array of fields. The Journal Impact Factor of journals cannot be compared between fields “because citation practices can vary significantly from one field to another” (Moed 2011).</p>
<p>The source normalized indicator of journal impact per paper, ‘<em>SNIP</em>’, includes refinements to address this problem; specifically it addresses that problem through the normalization of citations. SNIP normalizes the citation rates of articles against the average number of cited references per paper in that subject field (Moed 2010). In a simplistic example, if the articles in a field typically cite 10 articles, then a set of articles with a non-normalized citations rate of 12 would have a normalized citation rate of 1.2, which can be characterized as above average <em>for that field</em>.</p>
<p>To normalize away the differences in citation practices, however, requires properly delineating where those differences exist; in an article responding to criticisms of SNIP, Moed (2010) cites Garfield (1979) who reported that citation practices differ not only between fields, but between specialties and sub-specialties.</p>
<p>To ensure that a journal's citations are normalized against an appropriate field of research, within SNIP a particular journal's field is not determined by categorization but instead it is defined by what articles cite the journal. In other words, the “field” is the collection of all articles <em>outside</em> the journal that contain citations to articles <em>inside</em> the journal. (It should be noted that the term “article” is being used here loosely. A more accurate description might be “citable document”, since within SNIP “<em>articles</em>, <em>conference proceedings papers</em> and <em>reviews</em> are considered as fully fledged, peer-reviewed research articles” (Moed 2010).) In short, SNIP normalizes a journal's citations based on the field's average citations, where the field is defined by citations to the journal.</p>
<p>A version of SNIP that runs against the Scopus® database is available on the Journal Metrics website, <em>www.journalmetrics.com</em> (Elsevier 2015).</p>
<h4 id="sjr">SJR</h4>
<p>Another example of journal-impact metrics, the SCImago Journal Rank indicator (SJR), uses an implementation of social network analysis to rank journals. The resulting rankings indicate the relative prestige of the journals. This more gradated ranking is useful because more “poorly cited journals are entering the indices, [therefore] it is essential to have metrics that will allow one to distinguish with greater precision the level of prestige attained by each publication” (González-Pereira <em>et al.</em> 2010).</p>
<p>The SJR calculates for each journal its eigenvector centrality, which is “a measure of centrality… in which a unit's centrality is its summed connections to others, weighted by their centralities”, where ‘centrality’ is “network-derived importance” (Bonacich 1987) . For the SJR, therefore, ‘centrality’ is an indicator of a journal's prestige. The eigenvector centrality calculated for a journal is then normalized using the total number of citations to the journal, resulting in a size-independent rank (González-Pereira <em>et al.</em> 2010).</p>
<p>The resulting SJR metric is “aimed at measuring the current ‘average prestige per paper’ of journals for use in research evaluation processes” (González-Pereira <em>et al.</em> 2010).</p>
<h4 id="pagerank-eigenfactor.org">PageRank / Eigenfactor.org</h4>
<p>Another way to rank journals based on prestige instead of simple popularity, is the approach used by Eigenfactor.org®, which “ranks the influence of journals much as Google’s PageRank algorithm ranks the influence of web pages” (West 2015).</p>
<p>Bollen <em>et al.</em> (2006) have also used the PageRank methodology to rank journals by prestige, using “the dataset of the 2003 ISI Journal Citation Reports to compare the ISI [Impact Factor] and Weighted PageRank rankings of journals.” To achieve the ranking, citations are used as a basis to “iteratively” pass prestige from one journal to the other, until “a stable solution is reached which reflects the relative prestige of journals.” This is an adaptation of the original PageRank method, the difference being that connections between journals are weighted, based on citation frequencies.</p>
<p>Prestige and citation rankings were found to be largely similar, with two notable exceptions. The first were “journals that are cited frequently by journals with little prestige” that rank much lower on a prestige index than they do on a citation index. The second were the converse of the first, “journals that are not frequently cited, but their citations come from highly prestigious journals” so that they rank much higher on a prestige index than they do on a citation index.</p>
<p>Based on specific examples of these two types of journals that have distinctly different rankings when sorted by citations versus prestige, in general it is theory-heavy journals that have low citation counts, yet high prestige. The journals with high counts but low prestige are “methodological and applied journals” or ones “that frequently publish data tables” (Bollen <em>et al.</em> 2006).</p>
<p>It is, however, unclear if this result is an indication of the importance of including work on theory to produce high-quality research, or simply evidence that a weighted PageRank methodology is effective at maintaining previous perceptions of the relative values of theoretical and applied research.</p>
