<h4>Journal-level metrics are poor surrogates for research evaluation</h4>
<div class="copy">
  <p>
    The variety of variants to the Journal Impact Factor might at first seem to be an indication that there is a lot of competition to win the favor of librarians. Because, of course, journal-level impact metrics like the Journal Impact Factor were born of the desire to rank the importance of journals so that librarians will know which ones to have in their libraries.
  </p>
  <p>
    And while modern journal-impact bibliometrics may be only distantly related to the original work by Gross and Gross <a class="ref" onclick="fetchText('ajax/Gross_1927.html','##-Gross_1927')">(1927)</a>, they have the same focus: ranking the journals themselves. The leap seems to be huge, from journal rankings to judgements of the quality of a individual articles in the journals, but in practice, it's often unnoticed. For example, in the introduction to an article describing yet another variant on the Journal Impact Factor, the authors write:
  </p>
  <blockquote>
    <p>
      “The citedness of a scientific agent has for decades been regarded as an indicator of its scientific impact, and used to position it relative to other agents in the web of scholarly communications. In particular, various metrics based on citation counts have been developed to evaluate the impact of scholarly journals….” <a class="ref" onclick="fetchText('ajax/González-Pereira_2010.html','##-González-Pereira_2010')">(González-Pereira <em>et al.</em> 2010)</a>
    </p>
  </blockquote>
  <p>
    In other words, since the number of citations received by a researcher, or research group, can be a useful indicator of the quality of their research, it is useful to count the number of citations received by the <del>researchers</del> journals in which the researchers publish. The logic of which seems to be reversed: instead of extrapolating that good articles make the journals they are in better, we extrapolate that good journals somehow make the articles that are in them better.
  </p>
  <p>
    As Adler <em>et al.</em> put it:
  </p>
  <blockquote>
    <p>
      “…Instead of relying on the actual count of citations to compare individual papers, people frequently substitute the impact factor of the journals in which the papers appear. They believe that higher impact factors must mean higher citation counts. But this is often <em>not</em> the case! This is a pervasive misuse of statistics that needs to be challenged whenever and wherever it occurs.” <a class="ref" onclick="fetchText('ajax/Adler_2009.html','##-Adler_2009')">(Adler <em>et al.</em> 2009)</a>
    </p>
  </blockquote>
  <p>
    Yet, it's hard to explain the variety of bibliometrics available to assess the impact of journals, and the nature of some of the refinements that differentiate them, without assuming that the metrics are being used for something more generally desirable than the ranking of the journals themselves.
  </p>
  <p>
    At the same time, however, it is this “pervasive misuse” of journal-level metrics that has helped lead to the development of article-level metrics. (See below.)
  </p>
</div>
<aside class="references"></aside>
