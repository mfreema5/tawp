<h4 id="journal-impact-factor">Journal Impact Factor</h4>
<div class=copy">
  <p>
    All citation-based metrics can probably be traced back to a paper by Gross and Gross <a class="ref" onclick="fetchText('ajax/Gross_1927.html','08-Gross_1927')">(1927)</a>; Archambault and Larivière <a class="ref" onclick="fetchText('ajax/Archambault_2009.html','08-Archambault_2009')">(2009)</a> tap that article as the starting point for the Journal Impact Factor. Since the Journal Impact Factor the best known and possibly most widely used citation-based metric for research, its origins can act as an example for all journal-level metrics in general.
  </p>
  <h5 id="finding-the-indispensable-journals">Finding the “indispensable” journals</h5>
  <p>
    The question posed by Gross and Gross <a class="ref" onclick="fetchText('ajax/Gross_1927.html','08-Gross_1927')">(1927)</a> was, “What… scientific periodicals are needed in a college library?” They wanted to identify to which journals a library should subscribe, and to do so they used quantitative methods to make comparisons between journals, and those methods eventually led to the Journal Impact Factor, along with other related bibliometrics.
  </p>
  <p>
    Gross and Gross decided to use a quantitive method to evaluate journals, as opposed to merely asking an expert to compile a list of “indispensable” journals in order to avoid a list that was “seasoned too much by the needs, likes and dislikes of the compiler.” In other words, they used quantitative methods to minimize any single individual's biases from the evaluation of journals.
  </p>
  <h5 id="counting-citations-to-sort-journals">Counting citations to sort journals</h5>
  <p>
    The basic method pioneered by Gross and Gross, and repeated later by other researchers, was to select a pre-eminent journal or a reference that was generally accepted as key to the field at hand, and then compile and quantify the sources cited in that keystone reference/journal.
  </p>
  <p>
    This is an extremely simplified example: if the journal that was selected as the central reference for a field, “Field”, contained five citations to a “Journal A”, ten citations to a “Journal B”, and four citations to a “Journal C”, then for that field the journals would be ranked and reported something like this:
  </p>
  <ul>
    <li>Leading periodicals in Field
      <ul>
        <li>Journal B &ndash; 10</li>
        <li>Journal A &ndash; 5</li>
        <li>Journal C &ndash; 4</li>
      </ul>
    </li>
  </ul>
  <p>
    These early citation metrics were inherently field-specific, since they took data from a central reference to some particular field.
  </p>
  <h5 id="compiling-citations-for-multiple-fields">Compiling citations for multiple fields</h5>
  <p>
    Soon, though researchers began to cross-compile citation information from multiple journals, and then to include journals from multiple fields in those compilations. For example, Gregory <a class="ref" onclick="fetchText('ajax/Gregory_1937.html','08-Gregory_1937')">(1937)</a> compiled more than 27,000 citations from across 27 subfields in medicine.
  </p>
  <p>
    Gregory gives a concise explanation of the purpose of her metrics:
  </p>
  <blockquote>
    <p>
      The foregoing Tables (I-27) answer primarily the needs of the specialist in his attempt to keep adequately abreast of the literature in his field. Two further Tables have been compiled, to indicate to the medical library and the librarian (A), the indispensable periodicals for al the fields consulted; (B), a short list of essential periodicals in general medicine which cover a large amount of material. This latter list is designed primarily for the individual and for the small library.
    </p>
  </blockquote>
  <p>
    There were 27 “foregoing Tables”, one for each of the subfields, yet in Gregory's Herculean compilation, there were no comparisons made between those fields. This is a feature that would continue in similar metrics for decades: when multiple fields were compiled and evaluated at the same time, the results for each field were reported separately. Presumably this was because the intended audience for these metrics had no need for cross-field comparisons, since libraries are generally tasked to select the best journals for some particular field, and are not given carte blanche to choose which fields to support, and researchers are interested in their field, not all fields.
  </p>
  <p>
    The compilation of citations from Martyn and Gilchrist <a class="ref" onclick="fetchText('ajax/Martyn_1968.html','08-Martyn_1968')">(1968)</a> is cited by Archambault and Larivière <a class="ref" onclick="fetchText('ajax/Archambault_2009.html','08-Archambault_2009')">(2009)</a> as having a large influence on methods used in the Journal Impact Factor, and it too continues the practice of reporting of metrics for separate fields separately.
  </p>
  <p>
    But eventually, when journal impact factors become used as a way to judge the quality of research, this lack of cross-field comparisons will become an issue as institutions try to equitably evaluate the quality of research done by their faculty and staff in an array of fields. Ways to address this problem are offered by some journal-level as refinements to the methodology of the Journal Impact Factor. (More details below.)
  </p>
  <h5 id="reporting-ratios-instead-of-counts">Reporting ratios instead of counts</h5>
  <p>
    Another feature of the Martyn and Gilchrist <a class="ref" onclick="fetchText('ajax/Martyn_1968.html','08-Martyn_1968')">(1968)</a> compilation that was carried forward into the Journal Impact Factor, and other bibliometrics, was first proposed by Hackh <a class="ref" onclick="fetchText('ajax/Hackh_1936.html','08-Hackh_1936')">(1936)</a>: reporting a ratio instead of a count. Specifically, the ratio of the number of citations to the number of pieces that might be cited.
  </p>
  <p>
    Consider another extremely simplified example: if there were five citations to articles published in “Journal A” in the same year that “Journal A” published a total of 20 articles, the citation ratio would be “5:20”, or “0.25”.
  </p>
  <p>
    We can extend our earlier simplified example to include reporting ratios instead counts, and hopefully see why this is the preferred method:
  </p>
  <table>
    <thead>
      <tr>
        <th>Journal</th>
        <th>Citation count</th>
        <th>Citable pieces</th>
        <th>Ratio</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Journal A</td>
        <td>5</td>
        <td>20</td>
        <td>0.25</td>
      </tr>
      <tr>
        <td>Journal B</td>
        <td>10</td>
        <td>20</td>
        <td>0.50</td>
      </tr>
      <tr>
        <td>Journal C</td>
        <td>4</td>
        <td>10</td>
        <td>0.40</td>
      </tr>
    </tbody>
  </table>
  <ul>
    <li>Leading periodicals in Field
      <ul>
        <li>Journal B &ndash; 0.50</li>
        <li>Journal C &ndash; 0.40</li>
        <li>Journal A &ndash; 0.25</li>
      </ul>
    </li>
  </ul>
  <p>
    Despite having fewer citations than Journal A, Journal C has I higher ratio citations to citable pieces, and so is ranked higher. Using that ratio to rank journals is a way to prevent quantity from overwhelming quality.
  </p>
  <h5>And thus the Journal Impact Factor</h5>
  <p>
    This, then, is the core of how the Journal Impact Factor and a number of similar metrics work. Gather citation counts and citable pieces for each journal, calculate the ratios, and rank the journals.
  </p>
  <p>
    However, there are a number of potential problems with this method. Those and some others problems with the Journal Impact Factor are most easily explained by looking at a some other journal-level metrics that use refined versions of this same basic methods.
  </p>
</div>
<aside class="references">
  <p id="08-Archambault_2009"></p>
  <p id="08-Gregory_1937"></p>
  <p id="08-Gross_1927"></p>
  <p id="08-Hackh_1936"></p>
  <p id="08-Martyn_1968"></p>
</aside>
